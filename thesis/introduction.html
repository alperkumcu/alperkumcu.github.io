<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Introduction | Looking for Language in Space: Spatial Simulations in Memory for Language</title>
  <meta name="description" content="1 Introduction | Looking for Language in Space: Spatial Simulations in Memory for Language" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Introduction | Looking for Language in Space: Spatial Simulations in Memory for Language" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Introduction | Looking for Language in Space: Spatial Simulations in Memory for Language" />
  
  
  

<meta name="author" content="Alper Kumcu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="theoretical-background.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i><b>0.1</b> Abstract</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#acknowledgement"><i class="fa fa-check"></i><b>0.2</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statement-of-research-question"><i class="fa fa-check"></i><b>1.1</b> Statement of Research Question</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#definition-of-concepts"><i class="fa fa-check"></i><b>1.2</b> Definition of Concepts</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#spatial-indexing"><i class="fa fa-check"></i><b>1.2.1</b> Spatial indexing</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction.html"><a href="introduction.html#looking-at-nothing"><i class="fa fa-check"></i><b>1.2.2</b> Looking at nothing</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction.html"><a href="introduction.html#simulation"><i class="fa fa-check"></i><b>1.2.3</b> Simulation</a></li>
<li class="chapter" data-level="1.2.4" data-path="introduction.html"><a href="introduction.html#cognitive-offloading"><i class="fa fa-check"></i><b>1.2.4</b> Cognitive offloading</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#overview-of-thesis"><i class="fa fa-check"></i><b>1.3</b> Overview of Thesis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="theoretical-background.html"><a href="theoretical-background.html"><i class="fa fa-check"></i><b>2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="theoretical-background.html"><a href="theoretical-background.html#mind-recreated-simulations-in-imagery-memory-and-language"><i class="fa fa-check"></i><b>2.1</b> Mind, Recreated: Simulations in Imagery, Memory, and Language</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="theoretical-background.html"><a href="theoretical-background.html#mental-imagery-as-a-simulation"><i class="fa fa-check"></i><b>2.1.1</b> Mental imagery as a simulation</a></li>
<li class="chapter" data-level="2.1.2" data-path="theoretical-background.html"><a href="theoretical-background.html#memory-retrieval-as-a-simulation"><i class="fa fa-check"></i><b>2.1.2</b> Memory retrieval as a simulation</a></li>
<li class="chapter" data-level="2.1.3" data-path="theoretical-background.html"><a href="theoretical-background.html#simulations-in-language"><i class="fa fa-check"></i><b>2.1.3</b> Simulations in language</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="theoretical-background.html"><a href="theoretical-background.html#i-look-therefore-i-remember-eye-movements-and-memory"><i class="fa fa-check"></i><b>2.2</b> I Look, Therefore I Remember: Eye Movements and Memory</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="theoretical-background.html"><a href="theoretical-background.html#eye-movements-and-eye-tracking"><i class="fa fa-check"></i><b>2.2.1</b> Eye movements and eye tracking</a></li>
<li class="chapter" data-level="2.2.2" data-path="theoretical-background.html"><a href="theoretical-background.html#investigating-memory-with-eye-movements"><i class="fa fa-check"></i><b>2.2.2</b> Investigating memory with eye movements</a></li>
<li class="chapter" data-level="2.2.3" data-path="theoretical-background.html"><a href="theoretical-background.html#eye-movements-in-mental-imagery-and-memory-simulations"><i class="fa fa-check"></i><b>2.2.3</b> Eye movements in mental imagery and memory simulations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><i class="fa fa-check"></i><b>3</b> Experiment 1 - Simulating Space when Remembering Words: Role of Visuospatial Memory</a>
<ul>
<li class="chapter" data-level="3.1" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#motivation-and-aims"><i class="fa fa-check"></i><b>3.1</b> Motivation and Aims</a></li>
<li class="chapter" data-level="3.2" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#abstract-1"><i class="fa fa-check"></i><b>3.2</b> Abstract</a></li>
<li class="chapter" data-level="3.3" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#introduction-1"><i class="fa fa-check"></i><b>3.3</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#spatial-indexing-and-looking-at-nothing-automaticity"><i class="fa fa-check"></i><b>3.3.1</b> Spatial indexing and looking at nothing: automaticity</a></li>
<li class="chapter" data-level="3.3.2" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#spatial-indexing-and-looking-at-nothing-dynamicity"><i class="fa fa-check"></i><b>3.3.2</b> Spatial indexing and looking at nothing: dynamicity</a></li>
<li class="chapter" data-level="3.3.3" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#looking-at-nothing-and-visuospatial-memory"><i class="fa fa-check"></i><b>3.3.3</b> Looking at nothing and visuospatial memory</a></li>
<li class="chapter" data-level="3.3.4" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#role-of-eye-movements-in-memory-retrieval"><i class="fa fa-check"></i><b>3.3.4</b> Role of eye movements in memory retrieval</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#method"><i class="fa fa-check"></i><b>3.4</b> Method</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#participants"><i class="fa fa-check"></i><b>3.4.1</b> Participants</a></li>
<li class="chapter" data-level="3.4.2" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#materials"><i class="fa fa-check"></i><b>3.4.2</b> Materials</a></li>
<li class="chapter" data-level="3.4.3" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#apparatus"><i class="fa fa-check"></i><b>3.4.3</b> Apparatus</a></li>
<li class="chapter" data-level="3.4.4" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#procedure"><i class="fa fa-check"></i><b>3.4.4</b> Procedure</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#results"><i class="fa fa-check"></i><b>3.5</b> Results</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#measures"><i class="fa fa-check"></i><b>3.5.1</b> Measures</a></li>
<li class="chapter" data-level="3.5.2" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#mixed-effects-modelling"><i class="fa fa-check"></i><b>3.5.2</b> Mixed-effects modelling</a></li>
<li class="chapter" data-level="3.5.3" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#memory-performance"><i class="fa fa-check"></i><b>3.5.3</b> Memory performance</a></li>
<li class="chapter" data-level="3.5.4" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#looking-behaviour"><i class="fa fa-check"></i><b>3.5.4</b> Looking behaviour</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#discussion"><i class="fa fa-check"></i><b>3.6</b> Discussion</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#looking-at-previous-word-locations"><i class="fa fa-check"></i><b>3.6.1</b> Looking at previous word locations</a></li>
<li class="chapter" data-level="3.6.2" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#indexing-word-locations"><i class="fa fa-check"></i><b>3.6.2</b> Indexing word locations</a></li>
<li class="chapter" data-level="3.6.3" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#looking-at-nothing-and-visuospatial-memory-1"><i class="fa fa-check"></i><b>3.6.3</b> Looking at nothing and visuospatial memory</a></li>
<li class="chapter" data-level="3.6.4" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#looking-at-nothing-and-memory-performance"><i class="fa fa-check"></i><b>3.6.4</b> Looking at nothing and memory performance</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html"><a href="experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html#conclusion"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Looking for Language in Space: Spatial Simulations in Memory for Language</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<div id="statement-of-research-question" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Statement of Research Question</h2>
<p>The link between space, language and memory is one of the most intriguing topics is cognitive psychology. Decades of evidence has showed that language provides us with a framework to materialise and structure the relatively abstract notion of space (Bloom, Garrett, Nadel, &amp; Peterson, 1996; Carlson-Radvansky &amp; Logan, 1997; Majid, Bowerman, Kita, Haun, &amp; Levinson, 2004; Miller &amp; Johnson-Laird, 1976; Talmy, 1983). Thus, language influences how people perceive, think about and remember space (Levinson, 2003). The blooming fields of grounded-embodied cognition and the extended mind thesis have redefined the space-language-memory network and rekindled an interest about space with a novel perspective. Once seen as a central processing machine contained in the skull, the mind is now viewed as more of an interactive architecture extending onto the body and space. In accordance, burgeoning evidence suggests that space is not only a content but also a “medium” for language and memory (Mix, Smith, &amp; Gasser, 2010). Memories can be indexed, and abstract thoughts can be grounded in space. Further, comprehending language can give rise to non-linguistic, spatial experiences. Despite the abundance of demonstrative experiments, how spatial perception and cognition influence language and memory operations is yet to be defined. To this end, the present thesis explores robust, systematic and often surprising ways that space is involved in memory for language. In particular, this thesis is an attempt to define and systematise different characteristics of spatial engagements during retrieval of words from memory. Thereby, it aims to contribute to the understanding of the effect of space on memory for language.</p>
</div>
<div id="definition-of-concepts" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Definition of Concepts</h2>
<p>The key concepts examined in the present work are defined in this section. Thereby, the scope of the thesis is outlined.</p>
<div id="spatial-indexing" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Spatial indexing</h3>
<p>The human mind can anchor spatially-located information to external spatial locations. Simply put, the location of a visual item is encoded with the item itself. This behaviour is called as spatial indexing. Marr (1982) was among the first to argue that the visual system separates locations from visual features (what vs. where). He introduced the term place token to refer to representations of location. A place token indexes the locations of visual information at the early stages of visual processing. Pylyshyn (1989) operationalised the phenomenon of spatial indexing in an exhaustive model termed FINST. The model assumes that spatial indexing is a primitive, that is pre-attentive or a pre-cognitive mechanism, which precedes “higher” visual operations such as recognition of patterns. But importantly, spatial indices allow for stability of visual information by constructing stable representations of locations in a constantly changing visual world. Hence, an index keeps pointing to the same location even if the visual pattern moves across the retina. According to the model, therefore, spatial indexing is different from merely encoding the position of a feature because a spatial index makes it possible to locate the visual stimulus for further examination when the necessity arises (see also Ballard, Hayhoe, Pook, &amp; Rao, 1997). In a similar vein, Coslett (1999) proposes a spatial registration hypothesis. This hypothesis assumes that “all stimuli, even if not relevant to the task at hand, are automatically marked with respect to spatial location in egocentric coordinate systems” (p. 703). Registering a location entails the creation of a marker that specifies the coordinate of an object in relation with the other objects in the environment. According to spatial registering hypothesis, spatial indexing is limited with the capacity of visual attention.</p>
</div>
<div id="looking-at-nothing" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Looking at nothing</h3>
<p>Consider the following situation: You are in the middle of a maths exam, trying to solve a problem. The problem you engage requires the application of a formula that you fail to remember at that point. However, you remember that the instructor has previously used the blackboard to explain the formula in one of the classes. You remember that she has written the formula on the top left corner of the board. Of course, the blackboard has already been cleaned and is totally blank now. Still, you raise your head in the hope of a sudden recall and look at the previous, but now-blank location of the formula. You have just looked at “nothing” (Spivey &amp; Geng, 2000). Obviously, there is nothing on the board. What you are looking at is the spatial indice that represents the previous but now-blank location of the formula. In this respect, looking at nothing is a memory-guided behaviour. Your eyes have been oriented to the previous location of the formula because you have attempted to remember it. To be more precise, spatial indices tied to external visual and verbal information trigger eye movements when a mental representation is reactivated. Thus, when retrieving information from memory, people tend to exploit location-based indices and look at the seemingly uninformative, empty locations where the information originally occurred even if the location is irrelevant to the task. Looking at nothing follows spatial indexing in a typical memory task with encoding and retrieval stages. Individuals are expected to index the location of the information at encoding. Then, they are expected to look at the previous locations of the to-be-retrieved information during retrieval. A considerable number of empirical studies has documented looking at nothing (e.g., Altmann, 2004; Richardson &amp; Kirkham, 2004; Richardson &amp; Spivey, 2000; Spivey &amp; Geng, 2000). The link between mental representations and looking behaviour (de Groot, Huettig, &amp; Olivers, 2016; Ferreira, Apel, &amp; Henderson, 2008; Martarelli, Chiquet, Laeng, &amp; Mast, 2017; O’Regan, 1992; Renkewitz &amp; Jahn, 2012; D. C. Richardson, Altmann, Spivey, &amp; Hoover, 2009; D. C. Richardson &amp; Spivey, 2000; Scholz, Mehlhorn, &amp; Krems, 2011; Wantz, Martarelli, &amp; Mast, 2015) and whether looks to blank locations improve memory (Johansson, Holsanova, Dewhurst, &amp; Holmqvist, 2012; Johansson &amp; Johansson, 2014; Scholz, Klichowicz, &amp; Krems, 2018; Scholz, Mehlhorn, &amp; Krems, 2016) have received much attention in looking at nothing research.</p>
</div>
<div id="simulation" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Simulation</h3>
<p>In grounded-embodied cognition, a simulation is defined as a partial activation or reactivation of an original perceptual, motor, affective or introspective experience (Barsalou, 1999). A simulation can occur in the absence (offline) or upon the perception (online) of the original stimulus. A large body of neural and behavioural evidence indicates that major segments of cognition such as mental imagery, memory, language comprehension, consciousness, expertise and several cognitive performances such as facial mimicry, gesturing, reasoning and problem solving rely on simulations (see Dijkstra &amp; Post, 2015; Hesslow, 2011; Körner, Topolinski, &amp; Strack, 2016; Wilson, 2002 for reviews). The current thesis focuses on offline and online simulations of space in memory for language.
Perceptual symbols lie at the heart of simulation mechanism as they make it possible the (re)activation of sensorimotor experiences (Barsalou, 1999). Perceptual symbols are mental representations that represent conceptual knowledge in the mind. Crucially, perceptual symbols are represented in the same system as the perceptual states that produced them (Barsalou, 1999, p. 578). As a result, they reflect physical and thus, perceptual characteristics of the referents they stand for, and represent continuous, rich and multimodal phenomenological experience. That said, perceptual symbols “partly” represent their referents rather than being similar to “high-resolution video-clips” or “high-fidelity sound clips” (Zwaan, Stanfield, &amp; Yaxley, 2002). Based on these features, perceptual symbols are fundamentally different from physical symbols (Newell, 1980) asserted within the computational theories of mind (Fodor, 1975; Haugeland, 1985; Newell &amp; Simon, 1976; Putnam, 1960). In contrast to perceptual symbols, physical symbols are amodal, abstract and discrete units. That is, they do not reflect the perceptual modality of or do not resemble to the physical entities that they refer to as in 1 and 0’s in a computation environment (Harnad, 1990; Pylyshyn, 1986).</p>
<div id="offline-simulation" class="section level4" number="1.2.3.1">
<h4><span class="header-section-number">1.2.3.1</span> Offline simulation</h4>
<p>Consider that you need to remember a piece of conceptual information about cats (e.g., “What does a cat sound like?”). According to grounded-embodied cognition, in such a case, the human mind relies on the reactivation of perceptual, motor, affective or introspective experiences that are formed during previous interactions with a cat (e.g., how soft its fur is, how it smells, how you feel when you stroke it etc.). However, these experiences or states are not reinstated exactly on later occasions and “different contexts may distort activations of the original representations” (Barsalou, 1999, p. 584). Offline simulations have a situated character (Barsalou, 2003). For instance, they represent specific cats in specific situations rather than representing generic knowledge about cats. You might have noticed that remembering a piece of information about cats in the way described above bear similarities with remembering the location of a maths formula. In the latter case, the individual encodes conceptual information (the formula) along with a spatio-perceptual experience (perceiving the location of the formula on the blackboard). Later, when she needs to access the conceptual information through memory retrieval, she reactivates the perceptual experience associated with it. In other words, she “re-lives” the perceptual experience that she has during the encoding of the conceptual information. On this ground, looking at nothing can be understood as a reflection of spatial simulation that takes place in the absence of the original stimulus.</p>
</div>
<div id="online-simulation" class="section level4" number="1.2.3.2">
<h4><span class="header-section-number">1.2.3.2</span> Online simulation</h4>
<p>Perceptual, motor, affective or introspective experiences can be activated whenever an individual perceives a stimulus. For example, merely viewing a graspable object such as a cup, a knife or a frying pan simulates the potential act of grasping. In turn, brain regions associated with motor movements are activated (Chao &amp; Martin, 2000; Tucker &amp; Ellis, 1998). Similarly, remembering a Japanese kanji character stimulates motor activity in the areas that would be activated when actually writing the characters (Kato et al., 1999; Topolinski &amp; Strack, 2009). Merely listening to words that involve strong tongue movements when pronounced such as “birra” (beer in Italian) or “ferro” (iron in Italian) activates tongue muscles (Fadiga, Craighero, Buccino, &amp; Rizzolatti, 2002). Language comprehension gives rise to simulations in this manner (see Chapter 2.1.3).
To summarise, the fundamental difference between an offline and an online simulation is the existence of an external stimulus at the time of the simulation. An offline simulation is a recreation of previous sensorimotor activations without any stimulus when the stimulus is re-accessed. An online simulation is a sensorimotor activation upon the perception of a stimulus (see Chapter 2.1 for further clarification with examples).
There are other instances of a simulation mechanism in cognitive psychology. In social cognition, for instance, attributing mental states to others as in “mind reading” (i.e., theory of mind) (Gallese &amp; Goldman, 1998; Premack &amp; Woodruff, 1978) is thought to be a simulation based on mirror neurons that are activated merely by observing others (Caggiano et al., 1996; Gallese, Fadiga, Fogassi, &amp; Rizzolatti, 1996). This aspect of simulations is beyond the scope of the current thesis despite the possible overlaps between theory of mind and mental simulations (Shanton &amp; Goldman, 2010). Likewise, simulation within the context of this thesis does not refer to computer-based simulations.</p>
</div>
</div>
<div id="cognitive-offloading" class="section level3" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Cognitive offloading</h3>
<p>Cognitive agents can perform physical actions to outsource their cognitive work to the body and the world with the aim of reducing cognitive load (Wilson, 2002). This behaviour is called as cognitive offloading (Risko &amp; Gilbert, 2016). Making a shopping list instead of keeping items to buy in your mind is a typical example of such behaviour. Cognitive offloading is treated as a fundamental mechanism of extended cognition, which posits that there is not a strict border between the mind and the world with the mind “leaking” into the world in surprising ways (A. Clark, 1998; A. Clark &amp; Chalmers, 1998).
Cognition can be extended to different body parts during several cognitive problems such as using hands in finger-counting (Butterworth, 2005) and co-speech gestures<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> in communication (see Goldin-Meadow &amp; Wagner, 2005; Pouw, de Nooijer, van Gog, Zwaan, &amp; Paas, 2014 for reviews). Cognitive work can be offloaded directly onto the world as well. In particular, people can exploit their immediate space in an intelligent way to reduce their cognitive load. In such a case, space becomes a resource that must be managed, much like time or energy (Kirsh, 1995). For example, Kirsh and Maglio (1994) showed that in Tetris, participants solve perceptual and cognitive problems (e.g., judging whether a geometric piece can fit into a specific position) in the space by physically rotating the pieces on the screen rather than in their minds which would require mental rotation.
Cognitive offloading with eye movements is a special case in which both body (via eye movements) and environment are used at the same time. In a seminal study, for instance, Hayhoe, Bensinger and Ballard (1998) asked participants to copy a pattern of coloured blocks on a computer screen. Participants had to use a mouse to drag scrambled blocks from the source window to an empty workspace window. The colour of blocks in the pattern was changed at different points during the task. Results showed that participants launched frequent fixations to the original pattern when they were copying it into the workspace window rather than keeping the original pattern in mind. In this respect, looking at nothing can be conceptualised as a type of cognitive offloading in which eyes are used to index certain locations in space for subsequent use in line with information-gathering goals (Ballard et al., 1997).
There are two important aspects of cognitive offloading. First, cognitive offloading often results in betterment in performance. For example, children were found to be most accurate when they actively gesture to count (even when using a puppet to count for them) as compared to internal counting (Alibali &amp; Dirusso, 1999). Second, exploitation of body and space to reduce cognitive load is correlated with the cognitive demands coming from the task and internal, “biologic” cognitive capacity. That is, higher cognitive load and/or lower cognitive capacity result in more frequent cognitive offloading. For example, in Risko and Dunn (2015), participants were asked to remember a mixture of random letters in a traditional short-term memory task. They either had to rely on internal memory only or had the option to write down the presented information and thus, to externalise memory work. Results showed that use of external storage became more frequent as the number of letters in a string increased. Further, individual short-term memory capacity predicted the likelihood of writing down the to-be-remembered information. The functional role of cognitive offloading and the link between cognitive load/capacity and the tendency of offloading indicate that there is a systematic trade-off between internal and external processing (Schönpflug, 1986). In the face of costs of cognitive operations and limited cognitive sources, a successful cognitive agent seeks to “get the job done” in the easiest way possible by integrating internal with external processes in complex environments (A. Clark, 1989).</p>
</div>
</div>
<div id="overview-of-thesis" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Overview of Thesis</h2>
<p>The current thesis investigates activations and re-activations of space in memory for language. It contains seven chapters (including this one).
Chapter 2 includes the review of the literature. Chapter 2.1 focuses on the simulations in mental imagery, memory and language as the core mechanism of grounded-embodied cognition. Chapter 2.2 addresses the relation between eye movements and memory.
Chapter 3 contains the first experimental study in the form of a paper. This study investigates whether individuals with better visuospatial memory relies more on space and simulation of word locations through looks at previous but now-blank locations when retrieving words from memory.
Chapter 4 contains the second experimental study in the form of a paper. This study employs the experimental design developed in Chapter 3 to investigate whether words that are more difficult to maintain and retrieve from memory lead to more reliance on space and spatial simulations via looking at nothing. It also examines the contributions of word properties (e.g., word length, imageability, frequency etc.) to the looking behaviour and memory performance.
Chapter 5 contains the third experimental study in the form of an article. This is a lab-based norming study in which participants were asked to read 1439 concrete and abstract words and associate them with horizontal and vertical locations on a two-dimensional coordinate system.
Chapter 6 contains the forth experimental study in the form of a paper. This study uses spatially normed words from Chapter 5 to investigate how spatial locations suggested and simulated by word meanings affect recognition memory performance in relation with the physical locations of the words on the screen.
Finally, Chapter 7 (General Discussion) summarises findings and conclusions of the empirical studies and discusses them in the context of the grounded-embodied and extended approaches to memory and language. Additionally, it includes suggestions for future work.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Note that cognitive offloading is only one suggested use of co-speech gestures. Co-speech gestures are assumed to play other roles as well (Goldin-Meadow, 1999).<a href="introduction.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="theoretical-background.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
