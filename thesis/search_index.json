[["index.html", "Looking for Language in Space: Spatial Simulations in Memory for Language Preface 0.1 Abstract 0.2 Acknowledgement", " Looking for Language in Space: Spatial Simulations in Memory for Language Alper Kumcu July 2018 Preface 0.1 Abstract Grounded-embodied theories hold that language is understood and remembered through perceptual and motor simulations (i.e., activations and re-activations of sensorimotor experiences). This thesis aims to illustrate simulations of space in memory for language. In four experiments, we explored (1) how individuals encode and re-activate word locations and (2) how word meanings activate locations in space (e.g., bird - upward location). In the first part of the thesis (Experiment 1 and 2), we addressed the potential simulation of word locations by probing eye movements during memory retrieval (i.e., looking at nothing). In particular, we investigated why and when individuals need to rely on external memory support via simulation of word locations. Experiment 1 results reveal that the propensity to refer to the environment during retrieval correlates with individuals visuospatial memory capacity. That is, participants with worse visuospatial memory relied more on the environment; whereas, participants with better visuospatial memory relied more on the internal memory sources. Experiment 2 shows that words which are more difficult to remember and, particularly, words that are more difficult to visualise in mind lead to more reliance on the environment during word retrieval. Experiment 1 and 2 suggest that the opportunistic and efficient human mind switches between internal sources and external support as a function of the richness of internal sources and cognitive demands coming from the words to be remembered. The second part of the thesis (Experiment 3 and 4), focuses on spatial simulations triggered directly by words (i.e., language-based simulations). Experiment 3 is a norming study in which raters were asked to associate words with locations in space. Experiment 3 results demonstrate that there is a high degree of agreement among individuals when linking both concrete and abstract words to locations in space although there are no explicit conventions with regard to these associations. Ratings in Experiment 3 indicate potential locations of word-induced simulations. Normed words were used as stimuli in Experiment 4 in which recognition memory for words with spatial associations was probed. Experiment 4 results show that both language-based simulation of space and simulation of word locations dictate memory performance even if space is irrelevant and unnecessary for successful retrieval. In particular, words that were presented in incongruent locations as to the locations they imply (e.g., bird in a downward location) were remembered faster than words presented in congruent locations (e.g., bird in an upward location). Memory performance deteriorated whenever attention was shifted to the locations simulated with word meanings. Overall, the thesis specifies the mechanics of two different types of spatial simulation in language and their effects on memory. Results and their implications are discussed within the framework of grounded-embodied approaches to language and memory and the extended cognition. 0.2 Acknowledgement What a roller-coaster of emotions! The thesis you are reading now was made possible through scientific, mental and emotional support of many people, which makes me a very lucky researcher. Here, I would like to name a few, and I apologise in advance to all others that I forgot to mention. First and foremost, I would like to thank my supervisor Dr Robin L. Thompson for her never-ending support, meticulous supervision and invaluable insights. She shared my joy during success and she lifted me up during moments of anxiety (which I had a lot). Without her, I could not even imagine realising my dream of doing a PhD in psychology. Thank you for being a great person and a great supervisor. Language and Psychology Lab Group had a huge impact on me as a researcher and the present thesis. I would like to thank Professor Linda Wheeldon for never leaving my questions unanswered, Dr Steven Frisson for his contributions to the present work with his questions and constructive feedback, Dr Katrien Segaert for her support in mixed-effects modelling and for being a role model with her research and lively presentations, Dr Andrew Olson, Dr Andrea Krott and also Professor Sotaro Kita from University of Warwick for their helpful suggestions on the experiments reported in this thesis. My friends and colleagues here in UK deserve special thanks. Many thanks to Evi Argyriou for providing an inspiring example with her minimalist approach to research and presentation, Freya Atkins for being such a great (lab) mate, Mahmoud Elsherif for our fruitful debates about science and life in general, Isabella Fritz for our discussions in grounded-embodied cognition and R, Anneline Huck from University College London and Beinan Zhou for their support in collecting the pilot data, Linde Luijnenburg for her company at the British Library, Rachel Merchant for her support in recording the auditory stimuli, Kristian Suen for the teamwork in organising Language and Cognition Seminars (LanCoS) and Onur Usta for the library sessions and gallons of coffee. I would like to acknowledge Turkish Council of Higher Education and Hacettepe University for awarding me with a doctoral scholarship and funding my research. I am grateful to my colleagues and friends at Hacettepe University in Turkey; especially Dr Sinem Sancaktarolu Bozkurt, Dr Hilal Erkazanc Durmu, Dr Elif Ersözlü and Dr Sezen Ergin Zengin for smoothing out the funding process and for their lovely companionship. I would like to thank all the participants who kindly participated the experiments. It goes without saying that the research could not be conducted without them. I also would like to thank our omniscient post-grad administrator Parveen Chahal for answering millions of questions that I have asked during my years at the School of Psychology. Last but not least, I am deeply grateful to my mother and sister, who have sent their affection and support from miles away. The majority of the people I mentioned here is woman. A heartfelt appreciation to all women in science! "],["introduction.html", "1 Introduction 1.1 Statement of Research Question 1.2 Definition of Concepts 1.3 Overview of Thesis", " 1 Introduction 1.1 Statement of Research Question The link between space, language and memory is one of the most intriguing topics is cognitive psychology. Decades of evidence has showed that language provides us with a framework to materialise and structure the relatively abstract notion of space (Bloom, Garrett, Nadel, &amp; Peterson, 1996; Carlson-Radvansky &amp; Logan, 1997; Majid, Bowerman, Kita, Haun, &amp; Levinson, 2004; Miller &amp; Johnson-Laird, 1976; Talmy, 1983). Thus, language influences how people perceive, think about and remember space (Levinson, 2003). The blooming fields of grounded-embodied cognition and the extended mind thesis have redefined the space-language-memory network and rekindled an interest about space with a novel perspective. Once seen as a central processing machine contained in the skull, the mind is now viewed as more of an interactive architecture extending onto the body and space. In accordance, burgeoning evidence suggests that space is not only a content but also a medium for language and memory (Mix, Smith, &amp; Gasser, 2010). Memories can be indexed, and abstract thoughts can be grounded in space. Further, comprehending language can give rise to non-linguistic, spatial experiences. Despite the abundance of demonstrative experiments, how spatial perception and cognition influence language and memory operations is yet to be defined. To this end, the present thesis explores robust, systematic and often surprising ways that space is involved in memory for language. In particular, this thesis is an attempt to define and systematise different characteristics of spatial engagements during retrieval of words from memory. Thereby, it aims to contribute to the understanding of the effect of space on memory for language. 1.2 Definition of Concepts The key concepts examined in the present work are defined in this section. Thereby, the scope of the thesis is outlined. 1.2.1 Spatial indexing The human mind can anchor spatially-located information to external spatial locations. Simply put, the location of a visual item is encoded with the item itself. This behaviour is called as spatial indexing. Marr (1982) was among the first to argue that the visual system separates locations from visual features (what vs. where). He introduced the term place token to refer to representations of location. A place token indexes the locations of visual information at the early stages of visual processing. Pylyshyn (1989) operationalised the phenomenon of spatial indexing in an exhaustive model termed FINST. The model assumes that spatial indexing is a primitive, that is pre-attentive or a pre-cognitive mechanism, which precedes higher visual operations such as recognition of patterns. But importantly, spatial indices allow for stability of visual information by constructing stable representations of locations in a constantly changing visual world. Hence, an index keeps pointing to the same location even if the visual pattern moves across the retina. According to the model, therefore, spatial indexing is different from merely encoding the position of a feature because a spatial index makes it possible to locate the visual stimulus for further examination when the necessity arises (see also Ballard, Hayhoe, Pook, &amp; Rao, 1997). In a similar vein, Coslett (1999) proposes a spatial registration hypothesis. This hypothesis assumes that all stimuli, even if not relevant to the task at hand, are automatically marked with respect to spatial location in egocentric coordinate systems (p. 703). Registering a location entails the creation of a marker that specifies the coordinate of an object in relation with the other objects in the environment. According to spatial registering hypothesis, spatial indexing is limited with the capacity of visual attention. 1.2.2 Looking at nothing Consider the following situation: You are in the middle of a maths exam, trying to solve a problem. The problem you engage requires the application of a formula that you fail to remember at that point. However, you remember that the instructor has previously used the blackboard to explain the formula in one of the classes. You remember that she has written the formula on the top left corner of the board. Of course, the blackboard has already been cleaned and is totally blank now. Still, you raise your head in the hope of a sudden recall and look at the previous, but now-blank location of the formula. You have just looked at nothing (Spivey &amp; Geng, 2000). Obviously, there is nothing on the board. What you are looking at is the spatial indice that represents the previous but now-blank location of the formula. In this respect, looking at nothing is a memory-guided behaviour. Your eyes have been oriented to the previous location of the formula because you have attempted to remember it. To be more precise, spatial indices tied to external visual and verbal information trigger eye movements when a mental representation is reactivated. Thus, when retrieving information from memory, people tend to exploit location-based indices and look at the seemingly uninformative, empty locations where the information originally occurred even if the location is irrelevant to the task. Looking at nothing follows spatial indexing in a typical memory task with encoding and retrieval stages. Individuals are expected to index the location of the information at encoding. Then, they are expected to look at the previous locations of the to-be-retrieved information during retrieval. A considerable number of empirical studies has documented looking at nothing (e.g., Altmann, 2004; Richardson &amp; Kirkham, 2004; Richardson &amp; Spivey, 2000; Spivey &amp; Geng, 2000). The link between mental representations and looking behaviour (de Groot, Huettig, &amp; Olivers, 2016; Ferreira, Apel, &amp; Henderson, 2008; Martarelli, Chiquet, Laeng, &amp; Mast, 2017; ORegan, 1992; Renkewitz &amp; Jahn, 2012; D. C. Richardson, Altmann, Spivey, &amp; Hoover, 2009; D. C. Richardson &amp; Spivey, 2000; Scholz, Mehlhorn, &amp; Krems, 2011; Wantz, Martarelli, &amp; Mast, 2015) and whether looks to blank locations improve memory (Johansson, Holsanova, Dewhurst, &amp; Holmqvist, 2012; Johansson &amp; Johansson, 2014; Scholz, Klichowicz, &amp; Krems, 2018; Scholz, Mehlhorn, &amp; Krems, 2016) have received much attention in looking at nothing research. 1.2.3 Simulation In grounded-embodied cognition, a simulation is defined as a partial activation or reactivation of an original perceptual, motor, affective or introspective experience (Barsalou, 1999). A simulation can occur in the absence (offline) or upon the perception (online) of the original stimulus. A large body of neural and behavioural evidence indicates that major segments of cognition such as mental imagery, memory, language comprehension, consciousness, expertise and several cognitive performances such as facial mimicry, gesturing, reasoning and problem solving rely on simulations (see Dijkstra &amp; Post, 2015; Hesslow, 2011; Körner, Topolinski, &amp; Strack, 2016; Wilson, 2002 for reviews). The current thesis focuses on offline and online simulations of space in memory for language. Perceptual symbols lie at the heart of simulation mechanism as they make it possible the (re)activation of sensorimotor experiences (Barsalou, 1999). Perceptual symbols are mental representations that represent conceptual knowledge in the mind. Crucially, perceptual symbols are represented in the same system as the perceptual states that produced them (Barsalou, 1999, p. 578). As a result, they reflect physical and thus, perceptual characteristics of the referents they stand for, and represent continuous, rich and multimodal phenomenological experience. That said, perceptual symbols partly represent their referents rather than being similar to high-resolution video-clips or high-fidelity sound clips (Zwaan, Stanfield, &amp; Yaxley, 2002). Based on these features, perceptual symbols are fundamentally different from physical symbols (Newell, 1980) asserted within the computational theories of mind (Fodor, 1975; Haugeland, 1985; Newell &amp; Simon, 1976; Putnam, 1960). In contrast to perceptual symbols, physical symbols are amodal, abstract and discrete units. That is, they do not reflect the perceptual modality of or do not resemble to the physical entities that they refer to as in 1 and 0s in a computation environment (Harnad, 1990; Pylyshyn, 1986). 1.2.3.1 Offline simulation Consider that you need to remember a piece of conceptual information about cats (e.g., What does a cat sound like?). According to grounded-embodied cognition, in such a case, the human mind relies on the reactivation of perceptual, motor, affective or introspective experiences that are formed during previous interactions with a cat (e.g., how soft its fur is, how it smells, how you feel when you stroke it etc.). However, these experiences or states are not reinstated exactly on later occasions and different contexts may distort activations of the original representations (Barsalou, 1999, p. 584). Offline simulations have a situated character (Barsalou, 2003). For instance, they represent specific cats in specific situations rather than representing generic knowledge about cats. You might have noticed that remembering a piece of information about cats in the way described above bear similarities with remembering the location of a maths formula. In the latter case, the individual encodes conceptual information (the formula) along with a spatio-perceptual experience (perceiving the location of the formula on the blackboard). Later, when she needs to access the conceptual information through memory retrieval, she reactivates the perceptual experience associated with it. In other words, she re-lives the perceptual experience that she has during the encoding of the conceptual information. On this ground, looking at nothing can be understood as a reflection of spatial simulation that takes place in the absence of the original stimulus. 1.2.3.2 Online simulation Perceptual, motor, affective or introspective experiences can be activated whenever an individual perceives a stimulus. For example, merely viewing a graspable object such as a cup, a knife or a frying pan simulates the potential act of grasping. In turn, brain regions associated with motor movements are activated (Chao &amp; Martin, 2000; Tucker &amp; Ellis, 1998). Similarly, remembering a Japanese kanji character stimulates motor activity in the areas that would be activated when actually writing the characters (Kato et al., 1999; Topolinski &amp; Strack, 2009). Merely listening to words that involve strong tongue movements when pronounced such as birra (beer in Italian) or ferro (iron in Italian) activates tongue muscles (Fadiga, Craighero, Buccino, &amp; Rizzolatti, 2002). Language comprehension gives rise to simulations in this manner (see Chapter 2.1.3). To summarise, the fundamental difference between an offline and an online simulation is the existence of an external stimulus at the time of the simulation. An offline simulation is a recreation of previous sensorimotor activations without any stimulus when the stimulus is re-accessed. An online simulation is a sensorimotor activation upon the perception of a stimulus (see Chapter 2.1 for further clarification with examples). There are other instances of a simulation mechanism in cognitive psychology. In social cognition, for instance, attributing mental states to others as in mind reading (i.e., theory of mind) (Gallese &amp; Goldman, 1998; Premack &amp; Woodruff, 1978) is thought to be a simulation based on mirror neurons that are activated merely by observing others (Caggiano et al., 1996; Gallese, Fadiga, Fogassi, &amp; Rizzolatti, 1996). This aspect of simulations is beyond the scope of the current thesis despite the possible overlaps between theory of mind and mental simulations (Shanton &amp; Goldman, 2010). Likewise, simulation within the context of this thesis does not refer to computer-based simulations. 1.2.4 Cognitive offloading Cognitive agents can perform physical actions to outsource their cognitive work to the body and the world with the aim of reducing cognitive load (Wilson, 2002). This behaviour is called as cognitive offloading (Risko &amp; Gilbert, 2016). Making a shopping list instead of keeping items to buy in your mind is a typical example of such behaviour. Cognitive offloading is treated as a fundamental mechanism of extended cognition, which posits that there is not a strict border between the mind and the world with the mind leaking into the world in surprising ways (A. Clark, 1998; A. Clark &amp; Chalmers, 1998). Cognition can be extended to different body parts during several cognitive problems such as using hands in finger-counting (Butterworth, 2005) and co-speech gestures1 in communication (see Goldin-Meadow &amp; Wagner, 2005; Pouw, de Nooijer, van Gog, Zwaan, &amp; Paas, 2014 for reviews). Cognitive work can be offloaded directly onto the world as well. In particular, people can exploit their immediate space in an intelligent way to reduce their cognitive load. In such a case, space becomes a resource that must be managed, much like time or energy (Kirsh, 1995). For example, Kirsh and Maglio (1994) showed that in Tetris, participants solve perceptual and cognitive problems (e.g., judging whether a geometric piece can fit into a specific position) in the space by physically rotating the pieces on the screen rather than in their minds which would require mental rotation. Cognitive offloading with eye movements is a special case in which both body (via eye movements) and environment are used at the same time. In a seminal study, for instance, Hayhoe, Bensinger and Ballard (1998) asked participants to copy a pattern of coloured blocks on a computer screen. Participants had to use a mouse to drag scrambled blocks from the source window to an empty workspace window. The colour of blocks in the pattern was changed at different points during the task. Results showed that participants launched frequent fixations to the original pattern when they were copying it into the workspace window rather than keeping the original pattern in mind. In this respect, looking at nothing can be conceptualised as a type of cognitive offloading in which eyes are used to index certain locations in space for subsequent use in line with information-gathering goals (Ballard et al., 1997). There are two important aspects of cognitive offloading. First, cognitive offloading often results in betterment in performance. For example, children were found to be most accurate when they actively gesture to count (even when using a puppet to count for them) as compared to internal counting (Alibali &amp; Dirusso, 1999). Second, exploitation of body and space to reduce cognitive load is correlated with the cognitive demands coming from the task and internal, biologic cognitive capacity. That is, higher cognitive load and/or lower cognitive capacity result in more frequent cognitive offloading. For example, in Risko and Dunn (2015), participants were asked to remember a mixture of random letters in a traditional short-term memory task. They either had to rely on internal memory only or had the option to write down the presented information and thus, to externalise memory work. Results showed that use of external storage became more frequent as the number of letters in a string increased. Further, individual short-term memory capacity predicted the likelihood of writing down the to-be-remembered information. The functional role of cognitive offloading and the link between cognitive load/capacity and the tendency of offloading indicate that there is a systematic trade-off between internal and external processing (Schönpflug, 1986). In the face of costs of cognitive operations and limited cognitive sources, a successful cognitive agent seeks to get the job done in the easiest way possible by integrating internal with external processes in complex environments (A. Clark, 1989). 1.3 Overview of Thesis The current thesis investigates activations and re-activations of space in memory for language. It contains seven chapters (including this one). Chapter 2 includes the review of the literature. Chapter 2.1 focuses on the simulations in mental imagery, memory and language as the core mechanism of grounded-embodied cognition. Chapter 2.2 addresses the relation between eye movements and memory. Chapter 3 contains the first experimental study in the form of a paper. This study investigates whether individuals with better visuospatial memory relies more on space and simulation of word locations through looks at previous but now-blank locations when retrieving words from memory. Chapter 4 contains the second experimental study in the form of a paper. This study employs the experimental design developed in Chapter 3 to investigate whether words that are more difficult to maintain and retrieve from memory lead to more reliance on space and spatial simulations via looking at nothing. It also examines the contributions of word properties (e.g., word length, imageability, frequency etc.) to the looking behaviour and memory performance. Chapter 5 contains the third experimental study in the form of an article. This is a lab-based norming study in which participants were asked to read 1439 concrete and abstract words and associate them with horizontal and vertical locations on a two-dimensional coordinate system. Chapter 6 contains the forth experimental study in the form of a paper. This study uses spatially normed words from Chapter 5 to investigate how spatial locations suggested and simulated by word meanings affect recognition memory performance in relation with the physical locations of the words on the screen. Finally, Chapter 7 (General Discussion) summarises findings and conclusions of the empirical studies and discusses them in the context of the grounded-embodied and extended approaches to memory and language. Additionally, it includes suggestions for future work. Note that cognitive offloading is only one suggested use of co-speech gestures. Co-speech gestures are assumed to play other roles as well (Goldin-Meadow, 1999). "],["theoretical-background.html", "2 Theoretical Background 2.1 Mind, Recreated: Simulations in Imagery, Memory, and Language 2.2 I Look, Therefore I Remember: Eye Movements and Memory", " 2 Theoretical Background 2.1 Mind, Recreated: Simulations in Imagery, Memory, and Language 2.1.1 Mental imagery as a simulation Mental imagery is the ability to construct mental representations in the absence of external sensory stimulation. Thus, it is a quasi-phenomenal experience (N. J. T. Thomas, 1999, 2018a). That is, it resembles the actual perceptual experience but occurs when the appropriate external stimulus is not there. The ability to see with the minds eye without any sensory stimulation is a remarkable feature of the human mind. Mental imagery underlies our ability to think, plan, re-analyse past events or even fantasise events that may never happen (Pearson &amp; Kosslyn, 2013). Accordingly, mental images involve, alter or even replace the core operations of human cognition such as memory (Albers, Kok, Toni, Dijkerman, &amp; De Lange, 2013; Rebecca Keogh &amp; Pearson, 2011; Tong, 2013), problem-solving (Kozhevnikov, Motes, &amp; Hegarty, 2007) decision-making (Tuan Pham, Meyvis, &amp; Zhou, 2001), counter-factual thinking (Kulakova, Aichhorn, Schurz, Kronbichler, &amp; Perner, 2013), reasoning (Hegarty, 2004; Knauff, Fangmeier, Ruff, &amp; Johnson-Laird, 2003), numerical cognition (Dehaene, Bossini, &amp; Giraux, 1993) and creativity (LeBoutillier &amp; Marks, 2003; Palmiero et al., 2016). The human mind can mentally visualise not only visual but also nonvisual perceptions (Lacey &amp; Lawson, 2013) such as auditory mental imagery (e.g., imagining the voice of a friend or a song) (Lima et al., 2015) or motor mental imagery (e.g., mentally rehearsing a movement before actualising it) (Hanakawa, 2016). Mental images can arise from nonvisual modalities (particularly auditory or haptic) in congenitally blind individuals (Cattaneo et al., 2008). However, the literature of mental imagery is largely dedicated to metal imagery that is specifically visual (Tye, 1991). How do we imagine? By extension, what does a mental image look like? The format of mental images has been extensively discussed in the 70s and 80s with two camps: pictorial (depictivism) and propositional (descriptivism) imagery. The pictorial position (Kosslyn, 1973) holds that mental images are like pictures and there are spatial relations between the imagined objects. On the other hand, the propositional view (Pylyshyn, 1973) is that mental images are more like linguistic descriptions of visual scenes based on tacit knowledge about the world (i.e., implicit knowledge that is difficult to express explicitly such as the ability to ride a bike). Mental imagery under the treatment of descriptivism is more of an amodal, formal system. Whereas, depictivism offers a picture of mental imagery that appears more compatible with the mechanics of grounded-embodied cognition. However, neither of these approaches captures the true essence of grounded-embodied cognition because both of them depend on an information processing approach. In both cases, perceptual data flows inward to a passive cognitive agent (N. J. T. Thomas, 1999). On the other hand, grounded-embodied theories of cognition conceive that mental imagery is based on active perceptions and actions. Mental images are considered as mental representations reactivated through previous perceptions (Ballard et al., 1997). Consequently, mental imagery is a simulation itself (Barsalou, 1999). As a matter of fact, mental imagery is assumed to be the most typical example of the simulation mechanism in that there are certain similarities between the properties of a sensorimotor simulation and mental imagery (Markman, Klein, &amp; Suhr, 2008): First, mental images arise from perceptual representations. They are formed in the absence of the original perceptual stimulation. And lastly, a mental image is not an exact copy of the percept but rather, a partial recreation (Kosslyn, 1980). Within this view, the primary function of mental imagery is to simulate reality at will in order to access previous knowledge and predict the future (i.e., mental emulation) (Moulton &amp; Kosslyn, 2009). In order to verify that mental imagery is sensorimotor simulation, evidence showing similarities between perception and imagery is needed. This is indeed what the literature on mental imagery within the framework of grounded-embodied cognition indicates. For instance, an overwhelming body of neuroimaging evidence shows that similar brain regions are activated during perception and imagery stages (Cichy, Heinzle, &amp; Haynes, 2012; Ganis, Thompson, &amp; Kosslyn, 2004; Ishai &amp; Sagi, 1995; Kosslyn, Thompson, &amp; Alpert, 1997; OCraven &amp; Kanwisher, 2000). Behavioural studies further reveal the nature of the link between perception and imagery. As early as 1910, the psychologist Cheves Perkys experiments showed that visual mental images can supress perceiving real visual targets unconsciously (i.e., the Perky effect) (Craver-Lemley &amp; Reeves, 1992; Perky, 1910). In the original experiment, participants were asked to fixate a point on a white screen and visually imagine certain objects there such as a tomato, a book or a pencil etc. After a few trials, a real but a faint image (i.e., in soft focus) of the concerned object was projected onto the screen. Participants failed to distinguish between their imagined projections and the real percepts. Shortly, real images intermingled with the mental images. For instance, some participants reported their surprise when they imagined an upright banana rather than a horizontally oriented one they were attempting to imagine (N. J. T. Thomas, 2018b). The Perky effect indicates that mental imagery and visual perception draw on the same sources (see also Finke, 1980). In a similar study (Lloyd-Jones &amp; Vernon, 2003), participants saw a word (e.g., dog) accompanied by a line drawing of that object in the perception phase. In the imagery phase, participants made spatial judgements about the previously shown picture. Simultaneously, a picture distractor appeared on the screen during mental imagery. The picture distractor was either unrelated to the mental image of the previously shown object (e.g., dog - strawberry) or conceptually related (e.g., dog - cat). Response times in the judgement task were longer when participants generated a mental picture along with the perception of a conceptually related picture but not a conceptually unrelated picture. These findings suggest that imagery and visual perception share the same semantic representations. Mental images are also processed in similar ways as the actual images. In Borst and Kosslyn (2008), participants scanned a pattern of dots and then, an arrow was shown on the screen. Participants then decided whether the arrow pointed at a location that had been previously occupied by one of the dots. Results showed that the time to scan during imagery increased linearly as the distance between the arrow and the dots increased in perception. Further, participants who were better at scanning distances perceptually were also better at scanning distances across a mental image, suggesting the functional role of perception in mental imagery. Finally, eye movement studies have given considerable support to the simulation account of mental imagery with two key findings (see Laeng, Bloem, DAscenzo, &amp; Tommasi, 2014 for a review): First, eye movements during perception are similar to those during imagery (Brandt &amp; Stark, 1997; Johansson, Holsanova, &amp; Holmqvist, 2006). Second, the amount of overlap between eye movements during perception and imagery predicts the performance in imagery-related tasks (Laeng &amp; Teodorescu, 2002). Eye movements in mental imagery are further elaborated in Chapter 2.2.3. 2.1.2 Memory retrieval as a simulation A simulation account of memory views memory retrieval as a partial recreation of the past that often includes sensorimotor and contextual details of the original episode (see Buckner &amp; Wheeler, 2001; Christophel, Klink, Spitzer, Roelfsema, &amp; Haynes, 2017; Danker &amp; Anderson, 2010; Kent &amp; Lamberts, 2008; Pasternak &amp; Greenlee, 2005; Rugg, Johnson, Park, &amp; Uncapher, 2008; Xue, 2018 for comprehensive reviews and see De Brigard, 2014; Mahr &amp; Csibra, 2018; Marr, 1971 for theoretical discussions). Hence, memory retrieval can be thought as a simulation of encoding in a similar way to mental imagery being a simulation of perception. Indeed, it is known that mental imagery and memory operate on similar machinery as long as their perceptual modalities match (e.g., visual mental imagery - visual memory). In the original model of working memory, Baddeley and Hitch (1974) assumed that one function of the visuospatial sketchpad (i.e., the component of working memory responsible for the manipulation of visual information) is manipulating visual mental images. In support of this assumption, Baddeley and Andrade (2000) showed that visual and auditory mental imagery tasks disrupted visual and auditory components of working memory respectively; that is, visuospatial sketchpad and phonological loop (i.e., the component of working memory responsible for the manipulation of auditory information). Keogh and Pearson (2014) evidenced that individuals with stronger visual mental imagery also have greater visual working memory capacity but not verbal memory capacity (see also Keogh &amp; Pearson, 2011). Grounded-embodied cognition takes the link between mental imagery and memory one step forward: Memory not only involves mental imagery, but memory is mental imagery itself. In accordance, encoding corresponds to perception and retrieval corresponds to imagery. In this respect, Albers et al. (2013) presented strong evidence that working memory and mental imagery share representations in the early visual cortex (V1 - V3). Further, as Buckner and Wheeler (2001) noted assessments of visual mental imagery ability in patients with damage to visual cortex support the possibility that brain regions involved in perception are also used during imagery and remembering (De Renzi &amp; Spinnler, 1967; D. N. Levine, Warach, &amp; Farah, 1985). Mental time travel is a striking example of the role of imagery in memory (Corballis, 2009; Schacter, Addis, &amp; Buckner, 2007; Suddendorf &amp; Corballis, 2007; Szpunar, 2010). Mental time travel is a cognitive ability of episodic memory (i.e., conscious and explicit recollection of past events) and episodic future thinking through mental imagery. Thus, a mental time traveller can mentally project herself backwards in time to re-live (i.e., reconstruct) the past events and pre-live (i.e., predict) the possible future events (Suddendorf &amp; Corballis, 2007). In this respect, mental time travel can be considered as an intertemporal simulation (Shanton &amp; Goldman, 2010). Growing evidence has shown that episodic memory and simulation of future by mental imagery share a core neural network (i.e., default network) (see Schacter et al., 2012 for a review), suggesting that memory, mental imagery and thinking about future rest on the similar neural mechanisms. As in mental imagery, a simulation approach to memory underlines the correspondence between encoding and retrieval (Kent &amp; Lamberts, 2008). Mounting evidence illustrates that common neural systems are activated both in encoding and retrieval (Nyberg, Habib, McIntosh, &amp; Tulving, 2000; Wheeler, Petersen, &amp; Buckner, 2000). Crucially, the similarity between neural patterns during encoding and retrieval is often predictive of how well an experience is remembered subsequently (see Brewer, Zhao, Glover, &amp; Gabrieli, 1998; Wagner et al., 1998 for reviews). There is much evidence indicating that reinstated neural activations are specific to perceptual modality (visual vs. auditory), domain (memory for what - where) and feature (colour, motion or spatial location) (see Slotnick, 2004 for a review). For example, Wheeler, Petersen and Buckner (2000) gave participants a set of picture and sound items to study and then a recall test during which participants vividly remembered these items. Results demonstrated that regions of auditory and visual cortex are activated differently during retrieval of sounds and pictures. In a similar fashion, Goldberg, Perfetti and Schneider (2006b) asked participants whether a concrete word possesses a property from one of four sensory modalities as colour (e.g. green), sound (e.g., loud), touch (e.g., soft) or taste (e.g., sweet). Retrieval from semantic memory involving flavour knowledge as in the word sweet increased specific activation in the left orbitofrontal cortex which is known to process semantic comparisons among edible items (Goldberg, Perfetti, &amp; Schneider, 2006a). A number of studies supported a simulation account of memory with retrieval dependent on perceptions by showing temporal overlaps between encoding and retrieval (Kent &amp; Lamberts, 2008). There is not a strict temporal regularity between retrieval and encoding as far as the ERP evidence shows (Allan, Robb, &amp; Rugg, 2000). However, better memory performance was found in serial recall when retrieval direction (forward vs. backward) matched with the order in which the words were encoded in the first place (J. G. Thomas, Milner, &amp; Hanerlandt, 2003). More direct evidence for temporal similarity between encoding and retrieval comes from Kent and Lamberts (2006). Participants were instructed to retrieve different dimensions of faces such as eye colour, nose shape, mouth expressions etc. Results revealed that features that were quickly perceived were also quickly retrieved. In addition to the findings from the abovementioned research areas, the historical phenomena of state-dependent memory and context-dependent memory show that memory retrieval is simulation of the original event. An overlap between the internal state (e.g., mood, state of consciousness) or external context of the individual during encoding and retrieval leads to higher retrieval efficiency (S. M. Smith &amp; Vela, 2001; Ucros, 1989). In one such study, Dijkstra, Kaschak and Zwaan (2007) documented faster retrieval when body positions and actions during retrieval of autobiographical events were similar to the body positions and actions in the original events compared to when body positions and actions were non-congruent. For example, participants were faster to remember how old they were at a concert, if they were instructed to sit up straight in the chair and clap their hands several times during the retrieval. In another intriguing study (Casasanto &amp; Dijkstra, 2010), participants were instructed to tell their autobiographical memories with either positive or negative valence, while moving marbles either upward or downward, which was an apparently meaningless action. However, retrieval was faster when the direction of movement was congruent with the valence of the emotional memory in a metaphorical way (i.e., upward for positive and downward for negative memories). Lastly, eye movements provide plentiful evidence that retrieval is perceptual recreation of encoding (D. C. Richardson &amp; Spivey, 2000; Spivey &amp; Geng, 2000) and further, these simulations usually predict the success of the retrieval (Johansson &amp; Johansson, 2014; Scholz et al., 2018, 2016). Eye movements in memory simulations are further elaborated in Chapter 2.2.3. 2.1.3 Simulations in language Language is one of the most influential domains in showing the centrality of simulations in human cognition. The claim of simulation view of language is simple: Meaning centrally involves the activation of perceptual, motor, social, and affective knowledge that characterizes the content of utterances (Bergen, 2007, pp. 277-278). Thus, a simulation mechanism is essential to comprehend and remember language. Switch-cost effects are a clear demonstration of perceptual and affective (re)activation in language. In this paradigm, participants are asked to verify whether a property (e.g., blender) corresponds to a particular target modality (e.g., loud in the auditory modality). The effect is that participants are slower to verify a property in one perceptual modality (e.g., blender can be loud - auditory modality) after verifying a property in a different modality (e.g., cranberries can be tart - gustatory modality) than after verifying a property in the same modality (e.g., leaves can rustle - auditory modality) (Pecher, Zeelenberg, &amp; Barsalou, 2003). A switch-cost occurs between properties with positive and negative valence (e.g., couple can be happy, and orphan can be hopeless) (Vermeulen, Niedenthal, &amp; Luminet, 2007) and at the sentence level (e.g., A cellar is dark in visual modality - A mitten is soft in tactile modality) (Hald, Marshall, Janssen, &amp; Garnham, 2011). Similar switching costs occur when participants switch between actual modalities in perceptual tasks (Masson, 2015). Thus, findings reviewed above support the claim that language is rooted in perceptions and language comprehension can activate these perceptions. Importantly, the same priming effect was not elicited when participants verified semantically associated properties (e.g., sheet can be spotless, and air can be clean) as opposed to unassociated properties (e.g., sheet can be spotless, and meal can be cheap) (Pecher et al., 2003). This finding rules out the alternative, computational hypothesis that properties across all modalities are stored together in a single, amodal system of knowledge. Rather, they support perceptual roots of language processing and language-based simulations. 2.1.3.1 Mental simulations and situation models Simulations triggered with language are slightly different than the sensorimotor simulations that have been covered so far. Sensorimotor simulations in mental imagery and memory rely on actual sensorimotor experiences (e.g., playing a piano or perceptually encoding an episode). They take place in an offline manner, that is, when the agent needs to access perceptual/conceptual information in the absence of original stimulus. Whereas, language-based simulations are activated upon perceiving linguistic stimuli in an online manner. The subject (re)creates perceptual, motor, affective, introspective and bodily states not by actually experiencing them but through linguistic descriptions. Further, language can give rise to simulations of several abstract conceptualisations that go beyond these states. This type of simulation is usually referred to as a mental simulation (Zwaan, 1999). Mental simulations can extend into and affect subsequent perceptual/conceptual processing and memory retrieval (discussed below). It is reasonable to assume that online mental simulation evoked by language and offline simulation in memory and mental imagery share some common architecture. After all, both types of simulations originate from perceptual, motor, affective, introspective and bodily states. That said, the substantial difference between offline and online simulation is conscious effort. Mental simulations based on language are assumed to be inherently involved in language comprehension and thus, triggered automatically and unconsciously (Zwaan &amp; Pecher, 2012). Whereas, offline sensorimotor simulation in memory and mental imagery is often a consequence of effortful, resource-consuming and conscious processes as memory and mental imagery themselves. In line, there is little to no evidence that mental simulation is correlated with the strength of mental imagery (Zwaan &amp; Pecher, 2012). The idea of mental simulation via language stems from the discovery of mirror neurons (Caggiano et al., 1996; Gallese et al., 1996). Mirror neurons are activated in motor regions of the brain by merely observing others executing motor actions (Hari et al., 1998). In a similar fashion, neural correlates were found between the content of what is being read and activated areas in the brain (see Hauk &amp; Tschentscher, 2013; Binkofski, 2010; Pulvermüller, 2005 for exhaustive reviews and Jirak, Menz, Buccino, Borghi for a meta-analysis). In a pioneering study (Hauk, Johnsrude, &amp; Pulvermüller, 2004), participants saw action words referring to face, arm and leg (e.g., lick, pick and kick) in a passive reading task and then, moved their corresponding extremities (i.e., left or right foot, left or right index finger, or tongue). Results showed that reading action verbs activates somatotopic brain regions (i.e., regions corresponding to specific parts of the body) that are involved in the actual movements (see also Buccino et al., 2005). For example, reading the word kick or pick invokes activation in the specific regions of motor and premotor cortex that control the execution of leg and arm movements respectively. Critically, several fMRI (functional magnetic resonance imaging) studies showed that not only concrete words but also idiomatic expressions involving action words (e.g., John grasped the idea or Pablo kicked the habit) (see Yang &amp; Shu, 2016 for a review) and counterfactual statements (e.g., if Mary had cleaned the room, she would have moved the sofa) (de Vega et al., 2014) elicit similar somatotopic activation in brain. In addition to action words, words in different perceptual modalities activate brain regions associated with the concerned modalities as well. For example, reading odour-related words such as cinnamon, garlic or jasmine triggers activations in primary olfactory cortex, the brain region involved in the sensation of smells (González et al., 2006). Language-based simulations go beyond recreation of perceptual and motor experiences. It is well-documented that reading narratives can form situation models (mental models) in the minds of the readers (e.g., Speer, Reynolds, Swallow, &amp; Zacks, 2009). Situation models are integrated, situational mental representations of characters, objects and events that are described in narrative (JohnsonLaird, 1983; Kintsch &amp; van Dijk, 1978). They allow readers to imagine themselves in the story by taking the perspective of the protagonist (e.g., Avraamides, 2003). Consequently, situation models give rise to simulations of perceptual, motor and affective states and also abstract structures such as time, speed, space, goals and causations (Speed &amp; Vigliocco, 2016; Zwaan, 1999; Zwaan &amp; Radvansky, 1998). For instance, Zwaan, Stanfield and Yaxley (2002) evidenced that language comprehenders simulate what the objects described by language look like. In their study, participants read sentences describing an animal or an object in a certain location (e.g., egg in a carton vs. egg in a pan). Thus, the shape of the objects changed as a function of their location, but it is only implied by sentences (e.g., The egg is in the carton. - whole egg). Even though, a line drawing of the object matching with the shape implied in the previous sentence (e.g., a drawing of a whole egg) improved participants performance in retrieval of the sentences. Similar results were demonstrated for sentences that imply orientation (e.g., vertical - horizontal) (D. C. Richardson, Spivey, Barsalou, &amp; McRae, 2003), rotation (Wassenburg &amp; Zwaan, 2010), size (de Koning, Wassenburg, Bos, &amp; Van der Schoot, 2017), colour (Zwaan &amp; Pecher, 2012), visibility (Yaxley &amp; Zwaan, 2007), distance (Vukovic &amp; Williams, 2014) and number (Patson, George, &amp; Warren, 2014). Language can activate simulations of more abstract structures in the same manner. Simulation of time, in particular, is well-documented. For instance, longer chronological distance between two consecutively narrated story events denoted with an hour later as compared to a moment later leads to longer reading times (Zwaan, 1996). Reading times measured with eye movements were also shown to be longer when reading slow verbs (e.g., amble) than fast verbs (e.g., dash) (Speed &amp; Vigliocco, 2014). Similarly, Coll-Florit and Gennari (2011) found that judging the sensicality of sentences describing durative states (e.g. to admire a famous writer) took longer than non-durative states (e.g. to run into a famous writer). Several other abstractions can be mentally simulated in the readers mind. In one experiment, participants can access the concept of cake more easily when they previously read a sentence in which a cake is actually present (Mary baked cookies and cake) than when it is not (Mary baked cookies but no cake) (MacDonald &amp; Just, 1989). In another experiment, participants simulated the protagonists thoughts and they remembered and forgot what the character in the story remembered and forgot (Gunraj, Upadhyay, Houghton, Westerman, &amp; Klin, 2017). In Scherer, Banse, Wallbott and Goldbeck (1991), participants simulated the intended emotions that were cued in characters voices. Mental simulations via language, and situation models play important roles in numerous cognitive tasks transcending language comprehension. Most importantly, simulations are involved in memory for language. Johansson, Oren and Holmqvist (2018) reported that eye movements on a blank screen when participants were remembering a narrative reflected the layout of the scenes described in the text rather than the layout of the text itself. Zwaan and Radvansky (1998) assumed that successful retrieval of what is comprehended would necessarily involve the retrieval of simulations. In accordance with this assumption, there is evidence that the ability to restructure situation models have beneficial effects on memory performance (Garnham, 1981; Magliano, Radvansky, &amp; Copeland, 2012). 2.1.3.2 Simulation of space with language Space has a privileged status in human cognition. Coslett (1999) argues that the representation of space in the mind has a fundamental evolutionary advantage because information about the location of objects in the environment is essential for sustenance and avoiding danger. A large body of evidence indicates that young children show sensitivity to spatial concepts and properties starting from the infancy (e.g., Aguiar &amp; Baillargeon, 2002; Casasola, 2008; Frick &amp; Möhring, 2013; Hespos &amp; Rochat, 1997; McKenzie, Slater, Tremellen, &amp; McAlpin, 1993; Örnkloo &amp; Von Hofsten, 2007; Wishart &amp; Bower, 1982). There is also evidence suggesting that development of spatial cognition forms the foundation for subsequent cognitive structures such as mathematical aptitude (Lauer &amp; Lourenco, 2016), creativity (Kell, Lubinski, Benbow, &amp; Steiger, 2013) and notably, language (Levinson, 1992; Piaget &amp; Inhelder, 1969). As a result, there is good reason to assume that language and space are inherently interconnected through the course of cognitive development (e.g., Casasola, 2005; Haun, Rapold, Janzen, &amp; Levinson, 2011; Hespos &amp; Spelke, 2004). People use language when describing space and spatial language schematises space by selecting certain aspects of a scene while ignoring other aspects (Talmy, 1983). For instance, across conveys the information that the thing doing the crossing is smaller than the thing that is being crossed (Tversky &amp; Lee, 1998). However, it does not contain any information about the distance between these things or their shapes. Thereby, language forms spatial representations in mind (H. A. Taylor &amp; Tversky, 1992). On the other hand, space provides a rich canvas for representing abstraction. Many abstract conceptualisations such as time (Boroditsky &amp; Ramscar, 2002), valence (Meyer &amp; Robinson, 2004), power (Zanolie et al., 2012), numerical magnitude (Dehaene et al., 1993), happiness (Damjanovic &amp; Santiago, 2016), divinity (Chasteen, Burdzy, &amp; Pratt, 2010), health (Leitan, Williams, &amp; Murray, 2015) and self-esteem (J. E. T. Taylor, Lam, Chasteen, &amp; Pratt, 2015) are understood with space (e.g., powerful is up, more is up, happy is up etc.). Further, space constraints the use of language with gestures and in sign language (Emmorey, 2001; Emmorey, Tversky, &amp; Taylor, 2000). In support of this, both brain imaging (Carpenter, Just, Keller, Eddy, &amp; Thulborn, 1999) and behavioural (Hayward &amp; Tarr, 1995) evidence indicate that there are similarities between spatial and linguistic representations. Given the central position of space in human mind as briefly discussed above and the intrinsic links between language and space, spatial simulations in language deserve particular attention. Reading narratives can activate simulations of spatial descriptions in a text through situation models. For instance, objects that are described close to a protagonist in a narrative are accessed faster than the objects described as more distant (Glenberg, Meyer, &amp; Lindem, 1987; Morrow, Greenspan, &amp; Bower, 1987). In seminal work, Franklin and Tversky (1990) showed that situation models of space derived from text are similar to the representations of spatial experiences in the real-world and notably, have bodily constraints. Participants in the study read descriptions of scenes and objects in them. Then, they were asked to remember and locate certain objects in a three-dimensional environment. Results showed that objects on the vertical (i.e., head-feet) axis were retrieved faster than objects on the horizontal (i.e., left-right) and sagittal (i.e., front-back) axes. The findings indicate that space in language is simulated with an ego-centric perspective rather than an allocentric (i.e., object-centred) or a mental transformation perspective. If the participants took an allocentric perspective as in inspecting a picture (in which the subject is not immersed into the environment), all directions would have been equally accessible. On the other hand, if they mentally transformed the described environments, response times would have varied as a function of the mental movement needed to inspect each location. Accordingly, response times would have been shortest for the objects in front of the subject and the accessibility would have decreased in line with the angular disparity from the front. Objects behind the subject, for example, would have been the most difficult to access. Bias for the objects on the vertical dimension suggests that simulation of space with language is body-based. As Franklin and Tversky (1990, p. 64) discuss, the dominant position of a person interacting with the environment is upright due to a number of reasons: First, the perceptual world of the observer can be described by one vertical and two horizontal dimensions (i.e., left/right and front/back). Second, vertical dimension is correlated with gravity, which in an important asymmetric factor in perceiving spatial relations. Thus, vertical spatial relations generally remain constant with respect to the observer. Third, the ground and the sky present stationary reference points on the vertical axis. On the other hand, horizontal spatial relations change frequently. Thus, horizontal dimension depends on more arbitrary reference points, such as the prominent dimensions of the observers own body. In another experiment using a similar methodology (Avraamides, 2003), it was demonstrated that simulated ego-centric positions are not static but can be automatically updated whenever the reader/protagonist moves in the text, suggesting the motor basis of language. In a recognition memory task, Levine and Klin (2001) showed that a story characters current location was more active in the readers memory than her/his previous location (see Gunraj et al., 2017). Further, such spatial simulations remained highly accessible even several sentences after last mention, indicating the robustness of these spatial simulations. There are stable representational mappings between language and space at the sentence level as well. Richardson, Spivey, Edelman and Naples (2001) asked participants to read sentences involving concrete and abstract action verbs (e.g., lifted, offended). They were then asked to associate diagrams illustrating motions on the horizontal (left and right) and the vertical axis (up and down) with the sentences depicting motion events. Substantial agreement was found between participants in their preferences of diagrams for both concrete and abstract verbs within action sentences. For example, participants tended to attach a horizontal image schema to push, and a vertical image schema to respect. In a later study, it was evidenced that spatial simulation triggered by a verb affects other forms of spatial processing along the same axis both in a visual discrimination and a picture memory task (D. C. Richardson et al., 2003). Spatial simulations interfered with visual discrimination on the congruent axis and deteriorated performance; however, memory performance was facilitated when the picture to be remembered and the simulated orientation matched (see Effects of mental simulation below). The effect was shown for both concrete and abstract verbs. Not only orientation, but upward and downward motion on the vertical axis are simulated via language. In one study (Bergen, Lindsay, Matlock, &amp; Narayanan, 2007), subject nouns and main verbs related with up and down locations interfered with visual processing in the same location. However, the effect was shown in literal sentences implying real space (e.g., The ceiling cracked  downward movement for the subject noun, The mule climbed  upward movements for the main verb) but not in sentences implying metaphorical space (e.g., The prices rose). Bergen et al. (2007) argue that the comprehension of the sentence as a whole, and not simply lexical associations, yield spatial simulations. However, there is evidence that single words can also trigger simulation of space. Several abstract nouns such as tyrant (up) and slave (down) invoke simulations of metaphorical spatial locations (e.g., Giessner &amp; Schubert, 2007). There are numerous common nouns in language such as bird (up) and worm (down) which are associated with actual spatial locations (i.e., spatial iconicity). Words denoting spatial locations simulate perceptions of these locations in space. In Zwaan and Yaxley (2003), participants were presented word pairs with spatial associations (e.g., attic - basement) and asked to decide whether the words are semantically related. Results showed that word pairs in a reverse-iconic condition (i.e., basement above attic) were judged slower than word pairs in an iconic condition (i.e., attic above basement). In a similar fashion, it was shown that reading words that occur higher or lower positions in the visual field (e.g., head and foot) hinders the identification of visual targets at the top or bottom of the display (Estes, Verges, &amp; Barsalou, 2008). 2.1.3.3 Effects of mental simulations Simulation-based language understanding leads to two main effects on simultaneous or subsequent visual/conceptual processing: compatibility and interference (see Fischer &amp; Zwaan, 2008 for a review). The underlying idea is that if understanding an utterance involves the activation of perceptual, affective and motor representations; then perceptions, emotions and actions that are congruent with the content of the utterances should facilitate visual/conceptual processing and vice versa (Bergen, 2007). For example, the action-sentence compatibility effect demonstrates compatibility/interference resulting from motor simulations in language. In the study introducing the effect for the first time (Glenberg &amp; Kaschak, 2002), participants were presented sensible and non-sensible sentences (e.g., Boil the air) and were asked to judge whether the sentences made sense or not. Sensible sentences implied actions either toward the body (e.g., Open the drawer) or away from the body (e.g., Close the drawer). Response button for identifying the sentence as sensible (i.e., yes button) was either near or far from the participants bodies. Results showed that when the implied direction of the sentence and the actual action to press the button matched, participants were faster to judge the sensibility of the sentences. For example, the sentence, Open the drawer was processed faster when participants reached the yes button near them, an action that is comparable to opening a drawer. The effect was found not only for imperatives but also for descriptive sentences (Andy delivered the pizza to you - toward sentence / You delivered the pizza to Andy - away sentence). Notably, sentences describing abstract transfers (Liz told you the story - toward sentence / You told Liz the story - away sentence) elicited an action-sentence compatibility effect as well. An action-sentence compatibility effect extends to sign language, suggesting that the motor system is involved in the comprehension of a visual-manual language as well (Secora &amp; Emmorey, 2015). Notably, the congruency effect was found relative to the verbs semantics (e.g., You throw a ball - away) not relative to the actual motion executed by the signer and perceived by the participant (e.g., You throw a ball - toward). Along with that, there are meta-reviews and experimental evidence arguing that an action-sentence compatibility effect is generally weak (Papesh, 2015; but see Zwaan, van der Stoep, Guadalupe, &amp; Bouwmeester, 2012) or highly task-dependent (Borreggine &amp; Kaschak, 2006). In sum, the current status of the literature suggests that the factors modulating an action-sentence compatibility effect and in general, effects of language-based simulations should be further specified. Simulations can also interfere with language comprehension which results in a mismatch advantage. For example, Kaschak et al. (2005) demonstrated that participants judge the feasibility of motion sentences (e.g., The horse ran away from you) faster when they simultaneously view visual displays depicting motion in the opposite direction as the action described in the sentence (e.g., a spiral moving towards the centre). They concluded that visual processing and action simulation during language comprehension engage the same neural circuits; which, in turn leads to a mismatch advantage. Connell (2007) evidenced a mismatch advantage in the simulation of colour with language. Participants read sentences involving an object which can occur in different colours (e.g., meat can be red when raw and brown when cooked). They were then presented pictures of objects and they had to decide whether the pictured object had appeared in the preceding sentence. Colour of the objects sometimes matched with the descriptions in the sentences (e.g., John looked at a steak in the butchers window - red steak) and sometimes did not match (e.g., John looked at a steak in the butchers window - brown steak). Responses were faster when the colour of the object mismatched with the colour implied by the previous sentence. Why do some studies show a congruency advantage and others an incongruency advantage? This is an important question within the context of the present thesis (see Chapter 6). Kaschak et al. (2005) argue that there are two factors determining match or mismatch advantage in language-based simulations: (1) Temporal distance between the perceptual stimulus and the verbal stimulus to be processed. (2) The extent to which the perceptual stimulus can be integrated into the simulation activated by the content of the sentence. In support of the temporal distance assumption, Borreggine and Kaschak (2006) found that action-sentence compatibility effect arises only when individuals have enough time to plan their motor response as they process the sentence. According to Kaschak et al. (2005), if the verbal information must be processed simultaneously with the perceptual information, a congruency or incongruency advantage may occur, depending on whether linguistic information and perceptual stimulus can be integrated. To be more specific, a congruency advantage is expected if the linguistic and visual stimulus are comparable such as reading the sentence The egg is in the carton and seeing a line drawing of a whole egg (Zwaan et al., 2002). However, different perceptual and linguistic stimulus such as reading the sentence The horse ran away from you and seeing a spiral moving towards the centre or away from it (Kaschak et al., 2005) result in an incongruency advantage (see also Meteyard, Zokaei, Bahrami, &amp; Vigliocco, 2008) 2.2 I Look, Therefore I Remember: Eye Movements and Memory 2.2.1 Eye movements and eye tracking Eyes do not flow in a smooth fashion when engaged in visual tasks (Huey, 1908). If you were able to see your gaze on the page or on the digital screen right now, you would notice that your eyes shift from one word to the next as you are reading this sentence. Known as saccades, these jumps are rapid, short and repeated ballistic (i.e. jerk-like) movements which occur approximately three to four times every second. Saccades abruptly change the point of fixations, the periods of eye immobility in which visual or semantic information is acquired and processed (Purves, Augustine, &amp; Fitzpatrick, 2001; D. C. Richardson &amp; Spivey, 2004). In simple terms, individuals internalise the visual world during fixations that are executed between saccades (Bridgeman, Van der Heijden, &amp; Velichkovsky, 1994; Simons &amp; Rensink, 2005). Eye movements are fundamental to visual perception because visual system cannot process the huge amount of available information in the visual world at once. Thus, execution of eye movements allows us to see the world as a seamless whole, although we can only see one region at a time (Buswell, 1936; Yarbus, 1967) due to anatomical limitations (i.e., the total visual field that the human eye covers) and also, limited processing resources (Levi, Klein, &amp; Aitsebaomo, 1985; D. C. Richardson, Dale, &amp; Spivey, 2007). Fixations have two elemental measures: location and duration. Both measures are highly informative of ongoing cognitive operations. We can see a stimulus clearly only when it falls into the most sensitive area of the retina (i.e., fovea) (~2o or 3 to 6 letter spaces), which is specialised for high acuity visual perception (Mast &amp; Kosslyn, 2002; Yarbus, 1967). Thus, eye position (i.e., fixation location) gives valuable information about the location of the attentional spotlight (Posner, Snyder, &amp; Davidson, 1980). In other words, fixation location corresponds to the spatial locus of cognitive processing. On the other hand, fixation duration corresponds to the duration of cognitive processing of the material located at fixation (Irwin, 2004, p. 2). Longer fixations suggest higher cognitive load or higher attentional processing demands required by a material or task (Irwin, 2004). The underlying idea behind the link between cognition and fixation is known as eye-mind assumption (Just &amp; Carpenter, 1980), which simply posits that the direction of our eyes indicates the content of the mind (Underwood &amp; Everatt, 1992). Based on the location and duration of fixations, cognitive processes can be measured and evaluated objectively and precisely during the occurrence of the process in question. There is now a universal consensus on the value of eye movements and eye tracking as a methodology in the investigation of the human mind (e.g., Hyona, Radach, &amp; Deubel, 2003; Just &amp; Carpenter, 1980; Rayner, 1998; Rayner, Pollatsek, Ashby, &amp; Clifton, 2012; Reichle, Pollatsek, Fisher, &amp; Rayner, 1998; Theeuwes, Belopolsky, &amp; Olivers, 2009; Van der Stigchel et al., 2006). Eye tracking methodology provides detailed measures with regard to the temporal order of fixations and saccades, gaze direction, pupil size and time spent on pre-defined regions of the scene. Fixation duration in a certain location relative to other locations is used as the main measure of looking behaviour in the present thesis. Eye movements can be monitored in various different ways. A pupil corneal reflection technique, that is based on high-speed cameras and near infrared light, is the most advanced remote and non-intrusive eye tracking method as of today. An illuminator shines dispersed infrared light to one eye or both eyes. A high-speed video camera captures the infrared reflections coming from the pupil and cornea (i.e., the outer layer of the eye) and transforms them into high-resolution images and patterns pertaining to the position of the eye(s) at any given millisecond. Such an infrared eye tracker can record eye movements quite precisely. Precision offered by an eye tracker is indicated by temporal resolution (i.e., sampling rate) and spatial resolution. Sampling rate shows the frequency of which a tracker samples and determines the position of the eye at a given moment. For example, the eye tracker used in the present thesis (i.e., SR EyeLink 1000) operates at a sampling rate of 1000 Hz, which means that the position of the eye is measured 1000 times every second. Put differently, it produces one sample of the eye position per one millisecond. Spatial resolution refers to the angular distance between successive samples of eye position. Thus, an eye tracker with a higher spatial resolution can detect even the smallest eye movements in a certain interest area. SR EyeLink 1000 has a spatial resolution of 0.25o - 0.50o which means that it can detect and sample eye movements within an angular distance of 0.25o - 0.50o. There generally exists a spatial difference between the calculated location of a fixation and the actual one. This difference is expressed in degrees of visual angle and reflects the accuracy of eye tracking. If you draw a straight line from the eye to the actual fixation point on the screen and another line to the computed one, the angle between these lines gives the accuracy. Thus, a smaller difference means higher accuracy. Accuracy depends on the screen size and the distance between the participant and the screen. Visual angle is also used to calculate the size of the experimental stimulus as it refers to the perceived size rather than the actual size. These measures of data quality are reported in the methods section of each experiment in accordance with the eye tracking standards and good practices in literature (Blignaut &amp; Wium, 2014; Holmqvist, Nyström, &amp; Mulvey, 2012; D. C. Richardson &amp; Spivey, 2004). 2.2.2 Investigating memory with eye movements The role of eye movements in evidently visual tasks and processes such as visual perception (Noton &amp; Stark, 1971), reading (Rayner, 1998), visuospatial memory (Irwin &amp; Zelinsky, 2002), visual search (Rayner, 2009) and visuospatial attention (Van der Stigchel et al., 2006) has been widely investigated for many decades and is very well-documented. Eye movements have recently emerged as an alternative means in memory research complementing behavioural measures based on end-state measures (e.g., hit rate, hit latency etc.) (Lockhart, 2000) and brain-imaging studies (Fiser et al., 2016; Gabrieli, 1998; Rugg &amp; Yonelinas, 2003). It has been known for a long time that previous experience and knowledge of the observer can govern eye movements in addition to the physical properties of the scene and stimulus. For example, many early studies have reported that human observers tend to look at areas of a picture which are relatively more informative to them. Importantly, informativeness rating of a region is modulated by the previous knowledge of the participants in the long-term memory (Antes, 1974; Kaufman &amp; Richards, 1969; Mackworth &amp; Morandi, 1967; Parker, 1978; Zusne &amp; Michels, 1964). Similarly, Althoff and Cohen (1999) reported that previous exposure to a face changes the viewing behaviour and thus, eye movements. In their study, different patterns of eye movements emerged when participants viewed famous versus non-famous faces driven by recognition, fame rating and emotion labelling tasks. Participants made fewer fixations and fixation durations were shorter when viewing famous faces (now known as a repetition effect), which suggests lower cognitive load in processing previously experienced stimuli that can be retrieved from memory. Ryan, Althoff, Whitlow and Cohen (2000) took a similar approach: Participants viewed a set of real word images under three conditions: novel (i.e., seen once during the experiment), repeated (i.e., seen once in each block of the experiment) or manipulated (i.e., seen once in original form in the first two blocks and then seen in a slightly changed form in the final block). Participants made fewer fixations and sampled fewer regions when viewing repeated and manipulated scenes compared to novel scenes (i.e., repetition effect). Repetition effect speaks to the link between stability of mental representation and memory-guided eye movements. To illustrate, in Heisz and Shore (2008), the number of fixations gradually decreased with the number of exposures to the unfamiliar faces during a task. There was also evidence for another memory driven eye movement behaviour known as a relational manipulation effect: a higher proportion of total fixation time was dedicated to the manipulated regions in the scenes compared with repeated or novel scenes. Further, participants made more transitions into and out of the changed regions of the manipulated scenes than in unchanged (matched) regions of the repeated scenes. Similar paradigms based on eye movements were also used to study memory in non-human primates (Sobotka, Nowicka, &amp; Ringo, 1997), infants (Richmond, Zhao, &amp; Burns, 2015; Richmond &amp; Nelson, 2009) and special populations. For example, Ryan et al. (2000, Experiment 4) did not observe any difference in looking patterns between amnesic patients with severe memory deficits and a control group when both were viewing the repeated images. However, amnesic patients did not look longer at the altered regions when viewing manipulated images, suggesting that amnesia disrupts relational memory, i.e., memory for the relations among the constituent elements of an experience. Likewise, in Niendam, Carter and Ragland (2010), schizophrenic patients failed to detect image manipulation, which was shown with eye movements and even though the memory impairment was not evident in behavioural results. Studies reviewed above suggest relevance of eye movements in memory and importantly, advantages of eye tracking methodology over behavioural, response-based methodologies. (1) Memory-guided eye movements are mostly obligatory, that is, cannot be controlled. For instance, repetition effect reviewed above occurs regardless of the instruction (i.e., whether participants are told just to study all items for later, are explicitly told to pick out the familiar item, or are told to avoid looking at the familiar item) (Ryan, Hannula, &amp; Cohen, 2007, pp. 522-523). (2) Individuals launch memory-guided eye movements whether exposure comes from short term memory (i.e., within the experiment) or from long term memory (i.e., prior to the experiment). (3) Memory-guided eye movements precedes conscious recall. As stated by Hannula et al. (2010), eye movements can reveal memory for elements of previous experience without appealing to verbal reports and without requiring conscious recollection (see Spering &amp; Carrasco, 2015 for a comprehensive review; but see Smith, Hopkins, &amp; Squire, 2006). For instance, repetition effect occurs as early as the very first fixation to the item and thus, prior to the behavioural recognition response (Ryan et al., 2007). Similarly, in Henderson and Hollingworth (2003), gaze durations were reliably longer for manipulated scenes although participants failed to detect changes explicitly. To conclude, studies making use of eye movements are highly promising as a methodology. They can provide unique information about memory processes, which complement overt behavioural measures and brain imaging (e.g., Hannula &amp; Ranganath, 2009). In fact, eye movements are so representative of memory that mathematical models are able to predict the task that a person is engaged in (e.g. scene memorisation) from their eye movements using pattern classification (Henderson, Shinkareva, Wang, Luke, &amp; Olejarczyk, 2013). It should also be noted that eye movements in memory are not limited to fixation measures or saccadic trajectories. Variation in pupil size (e.g., pupil dilation) and blinks have been used to probe the ongoing processes during retrieval (Goldinger &amp; Papesh, 2012; Heaver &amp; Hutton, 2011; Kahneman &amp; Beatty, 1966; Mill, OConnor, &amp; Dobbins, 2016; Otero, Weekes, &amp; Hutton, 2011; Siegle, Ichikawa, &amp; Steinhauer, 2008; Van Gerven, Paas, Van Merriënboer, &amp; Schmidt, 2004; Vo et al., 2008). A well-established finding is that the pupil dilates as the retrieval becomes cognitively challenging (Goldinger &amp; Papesh, 2012; Kucewicz et al., 2018; Laeng, Sirois, &amp; Gredeback, 2012). 2.2.3 Eye movements in mental imagery and memory simulations As discussed in Chapter 2.1.1 and 2.1.2, there is mounting evidence showing the neural and behavioural similarities between memory and mental imagery (Albers et al., 2013; Rebecca Keogh &amp; Pearson, 2011). Concordantly, simulation theories of memory within grounded-embodied cognition highlight the connection between memory and mental imagery in that both processes are simulations in essence. That is, memory retrieval/mental imagery is a neural, perceptual and/or motor reinstatement of perception (Borst &amp; Kosslyn, 2008; Buckner &amp; Wheeler, 2001; De Brigard, 2014; Ganis et al., 2004; Kent &amp; Lamberts, 2008; Mahr &amp; Csibra, 2018; Michaelian, 2016b; Norman &amp; OReilly, 2003; Pasternak &amp; Greenlee, 2005; Shanton &amp; Goldman, 2010). Eye movements play a crucial role in the simulation thesis of memory and mental imagery because they can illustrate the behavioural reinstatements between perception/encoding and imagery/retrieval. The essential idea behind this imagery - eye movements - memory network holds that eye movements are stored in memory along with the visual representations of previously inspected images and they are re-enacted during memory and visual imagery (Mast &amp; Kosslyn, 2002). Long before the idea had been proven empirically, many researchers hinted at a possible similarity in saccades between visual perception and imagery (Hebb, 1968; Hochberg, 1968; Neisser, 1967; Schulman, 1983). Hebb (1968) was probably the first researcher who explicitly argued that if the mental image is a reinstatement of the perceptual process, it should include the eye movements (p. 470). Brandt and Stark (1997) provided direct empirical evidence for this argument by showing that people do move their eyes during mental imagery and the scanpaths (i.e., the sequential order of fixations and saccades, not only their spatial positions) are not random (see also Noton &amp; Stark, 1971 for more on scanpath theory). Instead, they bear striking similarities with the scanpaths during the perception of the original image (Foulsham &amp; Underwood, 2008; Underwood, Foulsham, &amp; Humphrey, 2009) Correspondence between the eye movements in perception and imagery was so robust that it was observed both for auditory (retelling a story) and visual stimuli (depicting a picture) and even when participants were in complete darkness and thus, without any visual information at all during imagery (Johansson et al., 2006). It seems reasonable to assume that spatiotemporal characteristics of visual perception are similar to the mental imagery as eye movements reflect the mental processes during visual inspection. Memory-guided eye movements are also informative in grounding of abstract concepts such as time. In Martarelli, Mast and Hartmann (2017), participants launched more rightward saccades during encoding, free recall and recognition of future items compared to past items (see also Hartmann, Martarelli, Mast, &amp; Stocker, 2014; Stocker, Hartmann, Martarelli, &amp; Mast, 2015). A majority of the studies investigating the ocular motility in mental imagery and memory have revolved around the role and functionality of eye movements. Whether these eye movements are merely epiphenomenal (i.e., an involuntary by-product of the imagery process) or play an important role and affect the imagery/retrieval performance is an important issue in that it directly taps into the primary question of nonvisual gaze patterns: Why do people move their eyes when forming mental images in the first place? Early studies (Kosslyn, 1980) discussed a potential advantage in vividness if non-random eye movements are systematically employed during mental imagery; yet, they failed to provide experimental evidence, which led to a premature conclusion: Oculomotor movements during imagery were regarded as a mere reflection of the visual buffer (Kosslyn, 1980, 1987). A visual buffer is a hypothetical unit which is responsible for holding visual information for a limited time. Nonvisual eye movements in mental imagery were assumed as an additional mechanism for presenting complex scenes on the visual buffer without overloading its capacity (Brandt &amp; Stark, 1997). Thus, eye movements were viewed as passively mirroring the attentional window over the target image during encoding to provide a solution for the cognitive load problem (Irvin &amp; Gordon, 1998). There is now increasing evidence that eye movements have a relatively more direct role in mental imagery and memory (Bochynska &amp; Laeng, 2015; Hollingworth &amp; Henderson, 2002; Laeng et al., 2014; Mäntylä &amp; Holm, 2006; Stark &amp; Ellis, 1981; Underwood et al., 2009; Valuch, Becker, &amp; Ansorge, 2013). For example, in Laeng and Teodorescu (2002), participants viewed an irregular checkerboard, similar to the one used by Brandt and Stark (1997) or a coloured picture. Then, they were asked to mentally imagine the visual stimuli as they were looking at a blank screen. Percentages of fixation time on certain interest areas and the order of scanning during perceptual phase (i.e., original image) and imagery phase (i.e., blank screen) were highly correlated. But importantly, the strength of relatedness between scanpaths predicted the vividness of mental imagery. More recent evidence indicates that what is perceptually simulated in memory retrieval or mental imagery is not the order of eye movements (i.e., scanpaths) but rather, the locations of perception. In a visual memory experiment, Johansson, Holsanova, Dewhurst and Holmqvist (2012) found no literal re-enactment during retrieval although suppression of eye movements hindered retrieval accuracy (cf., Bochynska &amp; Laeng, 2015). By challenging the scanpath theory, they deduced that eye movements during retrieval are functional but not one-to-one reactivation of the oculomotor activity produced during perception/encoding (see also Foulsham &amp; Kingstone, 2012 for similar results). Also, in Laeng and Teodorescu (2002), the participants who were not allowed to free scan during imagery phase (i.e., fixed gaze condition) did worse when they were asked to recall the original pattern, which was calculated by the number of squares corresponded to the location of a black square in the grid. Using a similar paradigm in visuospatial memory, Johansson and Johansson (2014) asked participants to view objects distributed in four quadrants at the encoding phase. Participants then listened statements about the direction of the objects (e.g., The car was facing left) and were asked to decide whether the statements are true or false. Results showed that participants who were free to look at a blank screen during retrieval had a superior retrieval performance than participants whose eye movements were constrained to a central fixation point. Further, participants whose eye movements were constrained to the previous locations of the objects were more accurate and faster than participants whose eye movements were constrained to a diagonal location as to the previous location of the concerned object. Studies reviewed above suggest that the human mind encodes eye movements not as they are but in the form of spatial indices, seemingly invisible spatial pointers in space (D. C. Richardson &amp; Kirkham, 2004; D. C. Richardson &amp; Spivey, 2000). Spatial indices link internal representations to objects in the visual world by tapping into space-time information and in turn, trigger eye movements to blank locations during retrieval to reduce working memory demands (Ballard et al., 1997). Therefore, there is no need for a literal recapitulation of gaze patterns because eye movements function as a scaffolding structure with the network of spatial indices for the generation of a detailed image. In other words, spatial indices in the environment which are internalised via eye movements complete the representations in the head resulting in a detailed mental image (Ferreira et al., 2008). In an alternative model, ORegan and Noë (2001) put forward that seeing is a way of acting and eye movements are visual representations themselves in a nod to ecological psychology (Gibson, 1979). To sum up, current evidence shows that oculomotor activity during memory and mental imagery is not limited to the reconstruction of the original: it is essential to generate mental images. Further, it seems that the role of eye movements is also beyond an automatic and involuntary distribution of limited cognitive sources between the oculomotor activity and memory to alleviate the mental load. Rather, eye movements might serve as an optional, situational strategy in situations where expanding could make a difference for solving the task (Hayhoe et al., 1998; Laeng et al., 2014; J. T. E. Richardson, 1979). In support of this assumption, many task-oriented vision studies have suggested that the eyes are positioned at a point that is not the most visually salient but is the best for the spatio-temporal demands of the job that needs to be done (Hayhoe &amp; Ballard, 2005, p. 189). Furthermore, there is also intriguing evidence that these strategic, opportunistic eye movements in goal-directed behaviour are guided by a dopamine-based reward system (Glimcher, 2003; Hikosaka, Takikawa, &amp; Kawagoe, 2000). Thus, eye movements during imagery and memory can be situational and adaptive according to the task demands. For example, in Laeng, Bloem, DAscenzo and Tommasi (2014) eye movements during mental imagery concentrated in the salient, information-rich parts of the original image (e.g., head region of an animal picture in the study). Here, it is important to underline that difficulty of the task seems to be the decisive factor. For instance, memory tasks requiring relatively low cognitive load would not need a detailed mental image of the original scene to be solved and thus, retrieval should be challenging in order to observe any memory advantage (Hollingworth &amp; Henderson, 2002; Laeng et al., 2014). "],["experiment-1-simulating-space-when-remembering-words-role-of-visuospatial-memory.html", "3 Experiment 1 - Simulating Space when Remembering Words: Role of Visuospatial Memory 3.1 Motivation and Aims 3.2 Abstract 3.3 Introduction 3.4 Method 3.5 Results 3.6 Discussion 3.7 Conclusion", " 3 Experiment 1 - Simulating Space when Remembering Words: Role of Visuospatial Memory 3.1 Motivation and Aims Spatial simulation within grounded-embodied cognition and cognitive offloading within the extended cognition were outlined and discussed in Chapter 1 and 2. Based on this theoretical background, this chapter describes an experimental study investigating a memory-based looking behaviour (i.e., looking at nothing) which is representative of spatial simulation and cognitive offloading. In general terms, this study aims to investigate how spatial location is simulated following the visual perception of words to support the retrieval of these words. 3.2 Abstract People tend to look at uninformative, blank locations in space when retrieving information. This gaze behaviour, known as looking at nothing, is assumed to be driven by the use of spatial indices associated with external information. In the present study, we investigated whether people form spatial indices and look at nothing when retrieving words from memory. Participants were simultaneously presented four nouns. Additionally, word presentation was sometimes followed by a visual cue either co-located (congruent) or not (incongruent) with the probe word. During retrieval, participants looked at the relevant, blank location, where the probe word had appeared previously, more than the other, irrelevant blank locations following a congruent visual cue and when there was no cue between encoding and retrieval (pure looking at nothing condition). Critically, participants with better visuospatial memory looked less at nothing, suggesting a dynamic relationship between so-called external and internal memory. Overall, findings suggest an automatic spatial indexing mechanism and a dynamic looking at nothing behaviour for words. Highlights Participants offloaded memory work onto the environment with eye movements when remembering visually and simultaneously presented single words. Worse visuospatial memory led to more reliance on the environment during retrieval. 3.3 Introduction The human mind can anchor spatially-located information to external spatial locations. This mechanism has been expressed within a visual processing model, where the location of an object is separated from the visual features of it (Marr, 1982). This view, expanded into an exhaustive spatial indexing model (Pylyshyn, 1989), assumes that the visual system is able to individuate spatial relations before discerning a visual pattern and immediately index the locations of such patterns. In a similar fashion, spatial registration hypothesis (Coslett, 1999) holds that perceived stimuli are coded with respect to their location in space. Location, therefore, is a critical constituent of our interactions with the world (van der Heijden, 1993). Within the spatial indexing (or spatial registering/encoding) model, spatial indices remain attached to a particular object independent of its movements and visual properties. Critically, spatiotemporal continuity (i.e., persistence of spatial tags over time) occurs even when the visual information disappears, as often manifested in mental imagery (e.g., Brandt &amp; Stark, 1997). Spatial indices tied to external visual and verbal information trigger eye movements when a mental representation is reactivated. Thus, when retrieving information from memory, people tend to exploit location-based indices and look at the seemingly uninformative, empty locations where the information originally occurred even if location is irrelevant to the task. This behaviour is known as looking at nothing (Spivey &amp; Geng, 2000). In their pioneering study, Richardson and Spivey (2000) documented the use of spatial information and looking at nothing in verbal memory. Four faces randomly appeared on different quadrants of a two by two grid along with four corresponding spoken facts (e.g., Shakespeares first plays were historical dramas; his last was the Tempest). On the next screen, a statement (e.g., Shakespeares first play was the Tempest) probed participants memory for verbal information. During retrieval, there were significantly more looks in the blank quadrant where the face associated with the probed semantic information had been when compared to other quadrants. Thus, people did not just look at any nothing when answering the questions. Rather, they looked at an invisible spatial index, which was previously allocated to the information (Spivey &amp; Geng, 2000). Looking at nothing may be best thought of as an interface between internal and external worlds. Ferreira, Apel and Henderson (2008) proposed an integrated memory architecture, where external cues and internal representations work hand in hand to retrieve information as efficiently as possible (see also Richardson, Altmann, Spivey, &amp; Hoover, 2009). More precisely, the integrated memory account combines visual/auditory and spatial information in the external world with visual, linguistic, spatial and conceptual counterparts in the mental world. When part of an integrated representation (linguistic information) is reactivated, the other parts (spatial information) are retrieved as well. In this regard, looking at nothing is also an example of spatial simulation (Barsalou, 1999) in that the spatial position where the information is presented is recreated when the information is needed again. Looking at nothing can also be thought as an example of efficient cognitive offloading (Risko &amp; Gilbert, 2016), in which the memory work is offloaded onto the world to minimise internal demands. In the current study, we addressed the looking at nothing triangle, which is composed of actual looking behaviour, spatial indices and mental representations to answer three questions (1) How automatic is spatial indexing? Do individuals automatically index the locations of short and briefly presented linguistic information (e.g., visually and simultaneously presented single words)? (2) How dynamic is spatial indexing and looking at nothing? Can spatial indices be updated with subsequent visual information and how does it affect looking behaviour? (3) Does everybody look at blank locations, or is looking at nothing modulated by certain cognitive capacities such as visuospatial memory span? 3.3.1 Spatial indexing and looking at nothing: automaticity Looking at nothing typically occurs under two retrieval conditions as shown in the previous studies: (1) People look at blank locations when remembering spoken linguistic information such as factual sentences (Hoover &amp; Richardson, 2008; D. C. Richardson &amp; Kirkham, 2004; D. C. Richardson &amp; Spivey, 2000; Scholz et al., 2018, 2011, 2016). As illustrated above, spoken linguistic information is explicitly associated with a visual object in this paradigm, which we term as explicit indexing. In turn, eyes revisit the previous locations of the object (associated with the information) when retrieving the spoken factual information. (2) Looking at nothing also occurs during retrieval of visually presented non-linguistic information such as single objects (Martarelli &amp; Mast, 2013; Spivey &amp; Geng, 2000), arrangement of multiple objects (Altmann, 2004; Johansson &amp; Johansson, 2014) or visual patterns (Bochynska &amp; Laeng, 2015; Laeng et al., 2014). In this case, locations are encoded along with the visual object(s) or patterns. In the current study, we adopted a different approach to examine the automaticity of spatial encoding of linguistic information. We showed participants four nouns on a grid simultaneously to study for a brief period of time. Then, an auditorily presented word (which could be either among the studied words or not) probed participants verbal recognition memory while participants were looking at a blank screen. If participants automatically encode location of the words as assumed in spatial indexing hypothesis (implicit indexing), they should display looking at nothing behaviour. In other words, we predict more fixations in the now-blank locations of the probe word during retrieval compared to the other, irrelevant blank locations. However, if explicit indexing is required for looking at nothing as shown in the previous studies, there should be same amount of spontaneous looks in the relevant and irrelevant blank locations. Word locations are encoded in reading (Fischer, 1999; but Inhoff &amp; Weger, 2005), writing (Le Bigot, Passerault, &amp; Olive, 2009) and complex cognitive tasks such as memory-based decision-making (Jahn &amp; Braatz, 2014; Renkewitz &amp; Jahn, 2012; Scholz, von Helversen, &amp; Rieskamp, 2015). However, to what extent spatial encoding is automatic is not clear. An automatic process is fast, efficient, unconscious, unintentional, uncontrolled (i.e., cannot be wilfully inhibited), goal-independent and purely stimulus-driven (i.e., cannot be avoided) (Hasher &amp; Zacks, 1979; Moors &amp; De Houwer, 2006). Based on the criteria of automaticity, there is evidence that spatial encoding is an automatic process (Andrade &amp; Meudell, 1993), an effortful process (Naveh-Benjamin, 1987, 1988) and a combination of both (Ellis, 1991). For instance, Pezdek, Roman and Sobolik (1986) reported that spatial information for objects are more likely to be encoded automatically as compared to words. If participants look at previous locations of the words they are asked to remember, this could provide evidence for the automaticity of spatial encoding in looking at nothing due to the specifics of the present experimental paradigm (i.e., implicit encoding and brief encoding time) and the nature of looking at nothing (i.e., an unintentional and efficient behaviour). 3.3.2 Spatial indexing and looking at nothing: dynamicity How stable are the spatial indices in looking at nothing? Can they be updated with subsequent visuospatial information? How does updated spatial indices guide eye movements during memory retrieval? Answers to these questions are critical to understand mechanics of spatial indexing and looking at nothing. Thus, we tested whether congruency or incongruency of visuospatial cues between encoding and retrieval stages affects spatial indexing and looking at nothing. There are studies examining the temporal stability of spatial indices. For example, in Wantz, Martarelli and Mast (2015), location memory for visual objects faded 24 hours after the initial encoding. In consequence, participants looked at relevant, blank locations immediately after the encoding, 5 minutes and 1 hour after the encoding but not after 24 hours. That said, less is known about the spatial stability of indices. In one study (D. C. Richardson &amp; Kirkham, 2004), looking at nothing was reported when the visual information that was associated with the to-be-retrieved information moved and thus, updated the spatial indices. Participants looked at the previous locations of the updated locations rather than the original locations of the previously encoded information, suggesting a flexible and a dynamic spatial indexing mechanism. In the current study, a visual cue (i.e., a black dot) that was irrelevant to the words and to the task itself was shown between encoding and retrieval stages. The cue was presented either in the same quadrant of the grid or a diagonal quadrant as to the location of the probe word at the encoding stage. There was also a third condition, in which, the participants did not see a cue at all. A plethora of studies on Simon effect (Simon &amp; Rudell, 1967) indicates that spatial congruency between the stimulus and the response results in faster and more accurate response even when the location is irrelevant to successful performance (see Hommel, 2011 for a review). In line, Vankov (2011) presented evidence for a Simon-like effect in spatial indexing and showed that compatibility of irrelevant spatial information benefits memory retrieval (see also Hommel, 2002; Wühr &amp; Ansorge, 2007; Zhang &amp; Johnson, 2004) Participants saw four objects on a 2 x 2 grid (e.g., a line drawing of a guitar, cat, camel and plane) at the encoding phase. Then, they were presented a word denoting either a new object or one of the studied objects (e.g., guitar) in one of the four locations as to the location of the target object; that is, in the same location, a vertical location (above or below the target object), a horizontal location (left or right of the target object) or a diagonal location. Participants were asked to remember whether the object denoted by the word appeared before. The fastest responses were found when the word cue appeared in the same location as to the target object. Participants were the slowest to respond when the word cue was in the diagonal location as to the target object. In the light of the abovementioned evidence, we predict that (in)congruency between the spatial code attached to the word and the spatial code attached to the visuospatial cue could modulate looking at nothing behaviour. A congruent cue is predicted to emphasize the original location of the probe word and thus, the spatial indice tagging it. In turn, fixations to the relevant, blank locations should be more frequent in congruent cue condition as to no cue condition. On the other hand, an incongruent cue could update the spatial code attached to the word and disrupt looking to blank locations by shifting participants attention to a diagonal location. Such a pattern would suggest that spatial indexing and looking at nothing for words are dynamic processes that are sensitive to the systematic manipulation of irrelevant visuospatial information. 3.3.3 Looking at nothing and visuospatial memory The link between mental representations and looking at nothing is critical. One position within the radical grounded-embodied cognition (Chemero, 2011) is that the world functions as an outside memory without the need for mental representations (ORegan, 1992). According to this view, the external memory store can be accessed at will through visual perception. As discussed above, the integrated memory account (Ferreira et al., 2008) represents an opposing position within a relatively traditional grounded-embodied approach. Accordingly, internal memory (mental representations) and so called external memory (i.e., the external world internalised via spatial indices and eye movements) work cooperatively in an efficient and goal-directed manner in looking at nothing. To be more precise, the opportunistic and efficient mind (D. C. Richardson et al., 2009) exploits external support whenever it needs to minimise internal memory load. In support of this assumption, there is evidence that short-term memory capacity is a reliable predictor of conscious and intentional use of environment in memory tasks (see Risko &amp; Gilbert, 2016 for a review). In one memory study (Risko &amp; Dunn, 2015), offloading (i.e., writing down to-be-retrieved information) was given as an option to the participants. Results revealed that participants with worse short-term memory wrote down the information rather than relying on the internal memory more frequently than the participants with better short-term memory. In looking at nothing, there is evidence that reliance on the environment increases/decreases in proportion to internal demands. For example, people tend to exhibit less looking at nothing as they are asked to study and recall the same sentences over and over again, suggesting less reliance on external cues as the task becomes easier through repetition (Scholz et al., 2011). Similarly, Wantz, Martarelli and Mast (2015) showed less looks to blank locations with repeated recall without rehearsal as mental representations stabilise in time. However, not much is known about how individual differences in internal memory map onto the differences in looking at nothing within the scope of integrated memory account. If the opportunistic and efficient cognitive system uses both internal and external cues to access memory traces (D. C. Richardson et al., 2009) and if external cues are used to relieve internal operations (Risko &amp; Dunn, 2015), people with relatively worse visuospatial memory should rely more on the environment during memory retrieval (and vice versa). A correlation between visuospatial memory capacity and looking at nothing could provide further evidence for the integrated memory system by disproving the world as an outside memory argument (ORegan, 1992) and consequently, radical grounded-embodied cognition. 3.3.4 Role of eye movements in memory retrieval Another fundamental issue is whether looks occur to blank regions that are associated with information facilitate the retrieval of this information. This issue taps into a seemingly simple question with regard to the very nature of memory-guided eye movements: Why do people look at nothing? Role and functionality of eye movements in memory retrieval have been highly controversial (see Ferreira et al., 2008; Mast &amp; Kosslyn, 2002; Richardson et al., 2009 for discussions). First studies did not present any evidence for improvement in memory with looks to blank spaces (Hoover &amp; Richardson, 2008; D. C. Richardson &amp; Kirkham, 2004; D. C. Richardson &amp; Spivey, 2000; Spivey &amp; Geng, 2000; Vankov, 2011). Initial failure to demonstrate memory enhancement lead to the preliminary conclusion that eye movements only co-assist the retrieval process as a by-product (Spivey, Richardson, &amp; Fitneva, 2004). There is now growing evidence that gaze position can play a functional role in memory retrieval. For example, Laeng and Teodorescu (2002) reported that participants who viewed an image and looked at the blank screen freely (free perception &amp; free retrieval) were more accurate in answering the retrieval questions those whose gaze were restricted to the central fixation point (free perception &amp; fixed retrieval) (see also Johansson, Holsanova, Dewhurst, &amp; Holmqvist, 2012; Laeng et al., 2014 for memory advantage in free gaze compared to fixed gaze). In a similar gaze manipulation paradigm, participants who were instructed to look at relevant, blank regions were more accurate in judging statements about visual objects (Johansson &amp; Johansson, 2014) and verbal information (Scholz et al., 2018, 2016) than the participants who were instructed to look at a diagonal location as to the original location of the object or object associated with verbal information. The current study was not designed to test the role of looking behaviour in memory. That is, eye gaze at retrieval was not manipulated as in the studies reviewed above. Rather, we analysed the functionality of looking at nothing by using the fixation percentage in the relevant quadrant (i.e., looking at nothing) as a predictor of hit rate and hit latency within mixed-effects models. If looks to the relevant, blank locations predict recognition memory for visually presented single words, it might provide tentative evidence for the facilitatory role of gaze position in memory. 3.4 Method 3.4.1 Participants The experiment was carried out with forty-eight students at the University of Birmingham (six males; Mage = 19.92, SD = 1.96, range: 18 - 27). 96% of the participants were psychology students. All participants were monolingual native speakers of British English as determined with the Language History Questionnaire (version 2.0; Li, Zhang, Tsai, &amp; Puls, 2013). Participants reported normal or corrected-to-normal vision, no speech or hearing difficulties and no history of any neurological disorder. They received either £6 (n = 12) or course credit (n = 36) for participation. All participants were fully informed about the details of the experimental procedure and gave written consent. Post-experiment debriefing revealed that all participants were naïve to the purpose of the experiment. No participant was replaced. 3.4.2 Materials There were 192 trials involving 864 unique nouns in total. Trials were evenly divided into two groups (n = 96) as experimental (positive probe) trials and fillers. Probe words in the experimental trials were among the four study words in the encoding phase, whereas a different, not seen, word was probed in fillers. Words in the experimental trials (n = 384) were drawn from the extensions of Paivio, Yuille and Madigan norms for 925 nouns (J. M. Clark &amp; Paivio, 2004). The word pool was filtered to exclude words shorter than 3 letters and longer than 6 letters. Imageability, frequency (the CELEX database; Baayen, Piepenbrock, &amp; Gulikers, 1995; and logarithmic values of occurrences per million in Kucera &amp; Francis, 1967), age of acquisition, concreteness, availability (Keenan &amp; Benjafield, 1994), length in letters and number of syllables were identified as major predictors of verbal memory (Rubin &amp; Friendly, 1986) and used to control the experimental stimuli. The subset was then grouped into quadruples and trial sets were identified. Words within quadruples were matched on age of acquisition, availability, concreteness, imageability, length in letters, log frequency and number of syllables (all SDs &lt; 2.00 and all SEs &lt; 1.00). Words were further controlled so that no word started with the same letter, rhymed or related semantically with any other in the quadruple. Monosyllabic, disyllabic and trisyllabic words were evenly distributed [e.g., (3, 3, 3, 3), (1, 2, 1, 2) or (3, 2, 3, 2) etc.]. The word in each trial set with the median imageability value was selected as the probe among four words leaving the others as distractors (see Rubin &amp; Friendly, 1986). Welchs t-tests revealed no significant difference between the probe and distractor words in in frequency, length in letters or number of syllables (all ps &gt; .05). Thus, any word among the four words in each trial set was as likely to be remembered as any other word. Words in filler trials were drawn from the Toronto Word Pool (Friendly, Franklin, Hoffman, &amp; Rubin, 1982). They were also controlled to develop a consistent stimuli set. Words were grouped into quintuples and matched on log frequency in CELEX database (all SDs &lt; 0.60 and all SEs &lt; 0.30). Finally, we formed 192 unique mathematical equations [e.g., (2*3) - (2+3) = 1] to present as memory interference between encoding and retrieval phases (see Conway &amp; Engle, 1996 for a similar design). Half of the equations were correct. Incorrect equations were further divided into two equal groups: The results were either plus or minus one of the correct result. 3.4.3 Apparatus Stimuli were presented on a TFT LCD 22-inch widescreen monitor operating at 60 Hz with a resolution of 1680 x 1050 pixels (501.7 mm x 337.4 mm). The monitor was placed 640 mm in front of the participant. A chin and forehead rest was used to reduce head movements. Participants eye movements were monitored using SR EyeLink 1000 (sampling rate: 1000 Hz, spatial resolution &lt; 0.5°, http://sr-research.com/eyelink1000.html). Viewing was binocular but only the left eye was monitored. Auditory material was produced by a native female speaker of British English in a sound attenuated room and recorded using Audacity (version 2.1.10, https://www.audacityteam.org). Participants responded (yes/no they had seen the word) by pressing one of two keys on a standard keyboard. Eye movement data were extracted using the SR EyeLink Data Viewer (version 2.4.0.198, https://www.sr-research.com/data-viewer/). No drift or blink correction procedure was applied. Data were analysed and visualised in R programming language and environment (R Core Team, 2017). Mixed-effects models were constructed with lme4 package (Bates, Mächler, Bolker, &amp; Walker, 2015). Significance values of the likelihood tests and coefficients in models were computed based on the t-distribution using the Satterthwaite approximation with lmerTest package (Kuznetsova, Brockhoff, &amp; Christensen, 2015). 3.4.4 Procedure Eye tracking started with a standard nine-point calibration and validation, which confirmed high data quality (average calibration error &lt; 1° and maximum calibration error &lt; 1.50°). As spelled out in detail below, each trial was composed of five consecutive phases: (1) fixation (2) encoding, (3) cueing, (4) interference and (5) retrieval (See Figure 3.1). The task was to decide whether an auditorily presented word had appeared before or not (i.e., yes/no verbal recognition memory test). As soon as the participants made yes/no judgement by hitting one of the response buttons, the trial ended, and a new encoding phase began. (1) Fixation: A fixation cross appeared at the centre of the screen for 500 ms. (2) Encoding: Participants were presented four words on a 2 x 2 grid for 1600 ms. Words (Times New Roman, font size = 40) were centrally placed in rectangular boxes (285 x 85 in pixels, 7.6° x 2.4° of visual angle). By using boxes during encoding and retrieval, we aimed to enrich the spatial context in order to evoke more reliance on the space and thereby, observe looking at nothing when remembering short verbal information as words (see Spivey &amp; Geng, 2000 for the effect of spatial context). (3) Cueing: A flashing black dot appeared in cue trials for 1000 ms either in the same (congruent cue) or in the diagonal quadrant (incongruent cue) as the original location of the probe word in the encoding phase. There was also a third condition where no cue was presented between encoding and interference. Cue condition was a within-subjects variable and three cue conditions were randomly presented in a session. That said, an equal number of random participants (n = 16) saw the same probe word with a congruent cue, an incongruent cue or without any cue. (4) Interference: Participants were exposed to retrospective memory interference which was irrelevant to the main task. We expected to push out old information (i.e., encoded words) from the episodic buffer (Baddeley, 2000) and encourage participants to depend on spatial indices for the retrieval of words without using explicit indexing (see Martarelli, Chiquet, Laeng, &amp; Mast, 2017 for a similar paradigm). Hence, participants were presented a mathematical equation and asked to identify whether the equation was correct or not within 10,000 ms. (5) Retrieval: The probe word was auditorily presented as participants looked at the blank grid with empty boxes. There was a 500 ms gap at the between the presentation of the blank retrieval screen and the sound file was played. Participants were asked to make an unspeeded yes/no judgement to determine whether they had seen the probe word among the four words shown in the encoding phase within 10,000 ms (or they timed-out). The order of trials and equations were fully randomised independent of each other. The location of all words in all conditions was counterbalanced with Latin Square design to control gaze biases so that each word appeared an equal number of times in each location of the grid. The experiment was divided into four equal blocks with 48 trials in each block and there was a short pause between blocks. A typical session lasted approximately 60 minutes, including consent and setting up the eye tracker. Overall accuracy in interference equations and in the recognition memory test for words were 86% and 81% respectively, suggesting that participants attended to the task with high concentration. Following the experiment, a computerized version of the Corsi block-tapping task (Corsi, 1972) operated on PEBL (Psychology Experiment Building Language, version 0.13, test battery version 0.7, http://pebl.org) (Mueller &amp; Piper, 2014) was used to measure visuospatial short-term memory. (Figure 3.1) Figure 3.1 A schematic illustration of the temporal order of events in an example trial showing three different cue conditions. In this example, the relevant quadrant is the top left location, where the probe word (i.e., CHIN) appears. 3.5 Results 3.5.1 Measures Results were analysed in two parts as memory performance and looking behaviour. Memory performance: Hit rate and hit latency rate were used as measures of memory performance. Hit rate was the proportion of yes trials to which the participants correctly responded yes. Hit latency was the time in milliseconds between the onset of auditory presentation of the probe word and correct keyboard response. Participants were not instructed to make speeded response in the current paradigm. Nevertheless, hit latencies were reported to verify and complement hit rate. Looking behaviour: Fixation percentage was used as the main gaze measure and dependent variable as in previous looking at nothing studies discussed above (e,g., Wantz et al., 2015). Fixation percentage (or fixation frequency) is the percentage of fixations in a trial falling within a particular interest area in proportion to total fixations in a trial. Thus, it was computed by dividing the number of fixations on each quadrant to the total number of fixations during the retrieval phase (see Wenzel, Golenia, &amp; Blankertz, 2016 for a similar computation and use of fixation frequency). Words in the study were of varying lengths and thus, had different presentation durations. Fixation percentage was purposefully chosen as it is immune to such differences in durations. Further, we assumed that fixations rather than the time spent on particular region (i.e., dwell time per quadrant) are important for the link between memory and eye movements. Fixation-based measures are reliable indicators of memory load and attention in a given location (e.g., Just &amp; Carpenter, 1980; Meghanathan, van Leeuwen, &amp; Nikolaev, 2015). Hence, we preferred fixation percentage over dwell time percentage as a more refined indicator of looking at nothing2. Accordingly, we expected that participants would fixate on the relevant quadrant to derive support from the environment. Four rectangular interest areas corresponding to the quadrants were identified. All interest areas were of the same size (502 x 368 in pixels, 13.4° x 10.6° of visual angle). They framed the rectangular boxes that words were presented in (see Figure 3.1) and were not contiguous (see Jahn &amp; Braatz, 2014 for a similar arrangement). Interest areas occupied 93.58% of the total screen area. A circular interest area with a diameter of 40 pixels (1.1° of visual angle) was defined at the centre of the grid. Participants head were positioned on a head and chin rest to minimise head movements and we assumed that looking at the centre was the baseline looking behaviour in contrast to the looking at the relevant quadrant. Negative correlation between looks to the centre and the relevant quadrant confirmed this inverse relationship; rs (46) = -.73, p &lt; .0001. Proportion of fixations accrued on the interest areas during the retrieval phase (from the onset of auditory presentation of the probe word until the participants response) were calculated. Fixations were a minimum duration of 40 ms. First fixations and fixations outside the interest areas (7.91%) were omitted. Only hits (i.e., correct responses) in yes trials were included in the fixation analyses. Fixation percentages allocated to the three quadrants that did not contain the target probe word were averaged into one and analysed against the relevant quadrant in which the probe word was seen. 3.5.2 Mixed-effects modelling Data were analysed using linear and binomial logit mixed-effects modelling. Visual inspections of residual plots did not reveal any obvious deviations from homoscedasticity or linearity. Linear models were fit for continuous target variables (hit latency and fixation percentage). Binomial models were fit for categorical target variables (hit rate) and with bobyqa optimiser to prevent non-convergence. Participants and items were treated as random effects to explain by-participant and by-items variation (Baayen, Davidson, &amp; Bates, 2008). We started fitting models by building the random effects structure and followed a maximal approach. That is, random effects were included as both random intercepts and correlated random slopes (random variations) as long as they converged and were justified by the data (Barr, Levy, Scheepers, &amp; Tily, 2013). Random intercepts and slopes were included even if they did not improve the model fit in order to control for possible dependence due to repeated measures or order effects. In particular, imageability and word length among the lexico-semantic variables were selected to add as random slopes as long as the models converge. Random effects structure was simplified step by step as per the magnitude of the contribution of a random effect to the explanation of the variation in the data. That is, the random effect with the weakest contribution was dropped first and if necessary, the structure was further reduced accordingly. Contribution of a fixed effect was investigated by comparing a full model containing the effect in question against a reduced model in which only that effect was removed, or a null model without any fixed effects. Compared models had the same random effects structure (Winter, 2013). 3.5.3 Memory performance Hit rate We analysed whether there was a difference in hit rate across congruent and incongruent cue conditions. Fixed effect was cue location with two levels (congruent and incongruent cue). Imageability was added as random slopes into participants. Imageability and word length were added as random slopes into items. Cue location did not improve the model fit when compared against a null model; 2(1) = 0.01, p = .91. In other words, participants retrieved the probe words in incongruent cue condition (mean hit rate = 81%) as accurately as congruent cue condition (mean hit rate = 81%). Cue location did not improve the model fit either when no cue condition (mean hit rate = 82%) was included; 2(2) = 0.48, p = .79. Hit latency Linear mixed-effects models were fit to identify any difference in hit latency between cue conditions. Fixed effect was cue location with two levels (congruent and incongruent cue). Imageability, word length and cue location were added as random slopes into participants. Imageability and word length were added as random slopes into items. As in hit rate, likelihood tests indicated that there was no difference in hit latency between congruent (mean hit latency = 1807.48 ms) and incongruent (mean hit latency = 1830.94 ms) cue condition; 2(1) = 1.47, p = .23. Results did not change when no cue condition (mean hit latency = 1842.84 ms) was included; 2(2) = 2.59, p = .27. Effect of visuospatial memory on memory performance The effect of visuospatial memory capacity of participants as measured by Corsi-block tapping test on hit rate and hit latency was examined. As reported above, we did not find any differences in hit rate or hit latency across cue conditions. Thus, mixed-effects models including all cue conditions were fit. Fixed effect was Corsi-block tapping score. Hit rate: Imageability was added as random slopes into participants. Corsi-block tapping score improved the model fit; 2(1) = 9.39, p = .002. Participants with better visuospatial memory retrieved the probe words from memory more accurately; B = 0.01, z = 3.19, p = .001. Hit latency: Word length were added as random slopes into participants. Imageability and word length were added as random slopes into items. Corsi-block tapping score did not improve the model fit; 2(1) = 0.55, p = .46. 3.5.4 Looking behaviour Looking at nothing for words First, we examined whether there was a difference in spontaneous looking times between relevant and irrelevant quadrants during the retrieval phase. The target variable was fixation percentage in the correctly answered yes trials. Fixed effect was quadrant with two levels (relevant and irrelevant quadrant). Imageability was added as random slopes into participants. Imageability and word length were added as random slopes into items. Likelihood tests showed that quadrant significantly improved the model fit; 2(1) = 22.85, p &lt; .0001. Overall, participants looked significantly more at the relevant quadrant as compared to the irrelevant quadrant when retrieving probe words from memory; B = 0.03, t = 4.78, p &lt; .0001. Effect of visuospatial interference on looking at nothing We examined looking at the relevant and the irrelevant quadrants within congruent, incongruent and no cue conditions separately to specify the effect of visuospatial interference on looking at nothing (see Figure 3.2). The target variable was fixation percentage in correctly answered yes trials. Fixed effect was quadrant with two levels (relevant and irrelevant quadrant). Imageability was added as random slopes into participants; imageability and word length were added as random slopes into items in all models. Congruent cue condition: Quadrant improved the model fit; 2(1) = 27.51, p &lt; .0001. Participants looked significantly more at the relevant quadrant compared to the irrelevant quadrant in congruent cue condition; B = 0.05, t = 5.25, p = .0001. No cue condition: Quadrant improved the model fit with a smaller magnitude compared to congruent cue condition; 2(1) = 5.00, p = .03. Participants looked significantly more at the relevant quadrant compared to the irrelevant quadrant in no cue condition; B = 0.02, t = 2.24, p = .03. Incongruent cue condition: Quadrant did not improve the model fit; 2(1) = 0.61, p = .43. Participants did not look significantly more at the relevant quadrant during the retrieval phase; B = 0.007, t = 0.78, p = .43. Models including quadrant with three levels (i.e., relevant and irrelevant quadrant and central interest area) indicated that participants did not look at any region more than the other (ps &gt; .05) in incongruent cue condition. Difference between conditions: Models with fixation percentage in the relevant quadrant as the target variable were fit by taking the no cue condition as the baseline condition. Results revealed that participants looked at the relevant, blank locations more frequently in congruent cue condition as to no cue condition; B = -0.03, t = 2.48, p = .01. There was not such a difference between no cue and incongruent cue (p = .95). (Figure 3.2) Figure 3.2 Proportion of fixations in the relevant and irrelevant quadrants across three cue conditions. Values on the y axis correspond to percentages (e.g., 0.1 = 10%). Notched box plots show median (horizontal line), mean (yellow dot), 95% confidence interval of the median (notch), interquartile range (the box), the first and the third quartiles (lower and upper ends of the box) and ranges (vertical line). Grey dots represent data points. * p  .05, **** p  .0001 Visuospatial memory and looking behaviour Visuospatial memory as a predictor of looking at nothing The effect of visuospatial memory capacity of participants as measured by Corsi-block tapping test on looking at the relevant, blank quadrant was investigated. The target variable was fixation percentage in the relevant quadrant in correctly answered yes trials. Fixed effect was Corsi-block tapping score. Word length was added as random slopes into participants; imageability was added as random slopes into items in all models. Congruent cue condition: Corsi-block tapping score did not predict looks in the relevant quadrant; 2(1) = 1.07, p = .30 or irrelevant quadrant; 2(1) = 1.50, p = .22. No cue condition: Corsi-block tapping score did not predict looks in the relevant quadrant; 2(1) = 0.30, p = .58 or irrelevant quadrant; 2(1) = 1.52, p = .22. Incongruent cue condition: Corsi-block tapping score improved the model fit for fixation percentage in the relevant quadrant; 2(1) = 4.83, p = .03 but not the irrelevant quadrant; 2(1) = 0.67, p = .41. Participants with better visuospatial memory looked less at the relevant quadrant during memory retrieval when there was an incongruent cue between encoding and retrieval phases; B = -0.0009, t = 2.28, p = .03. Correlation between visuospatial memory and looking behaviour We tested the correlation between visuospatial memory as a function of Corsi-block tapping test and fixations to relevant, irrelevant and central interest areas under three different cue conditions (see Figure 3.3). Relevant quadrant: There was a significant, negative correlation between visuospatial memory capacity and fixation percentage in the relevant quadrant under incongruent cue condition; rs (46) = -.37, p = .009. Participants with better visuospatial memory tended to look less at the relevant quadrant when there was an incongruent cue between encoding and retrieval phases. There was not such a correlation within congruent; rs (46) = .20, p = .18 or no cue conditions; rs (46) = -.18, p = .22. Irrelevant quadrant: There was a significant, negative correlation between visuospatial memory capacity and fixation percentage in the irrelevant quadrant under the no cue condition; rs (46) = -.29, p = .05. Participants with better visuospatial memory tended to look less at the irrelevant quadrant when there was not any cue between encoding and retrieval phases. There was not such a correlation within congruent; rs (46) = -.26, p = .07 or incongruent cue conditions; rs (46) = -.16, p = .27. Central interest area: There was a significant, positive correlation between visuospatial memory capacity and fixation percentage in the central interest area under congruent cue condition; rs (46) = .39, p = .006, no cue condition; rs (46) = .30, p = .04 and incongruent cue condition; rs (46) = .33, p = .02. All conditions combined, participants with better visuospatial memory tended to look more at the central interest area; rs (46) = .30, p = .04. (Figure 3.3) Figure 3.3 Scatterplots showing the correlations between visuospatial memory as a function of Corsi-block tapping score (higher score means better visuospatial memory) and fixation percentage in the relevant, irrelevant and central interest area. Values on the y axis correspond to percentages (e.g., 0.25 = 25%). Scatterplot has a linear regression line. Blue band around the line represents 95% confidence interval. Tassels at the x and y axis illustrate the marginal distribution of data along visuospatial memory and fixation percentage. Functionality of looking at nothing The current experiment was not designed to test the functionality of looking behaviour in memory. Nevertheless, we examined whether memory performance (hit rate and hit latency) was predicted by the proportion of fixations in the relevant, blank locations. Imageability was added as random slopes into participants. Imageability and word length were added as random slopes into items. Looks to relevant, blank locations predicted hit rate; B = 0.54, z = 2.28, p = .02 in congruent cue but not in no cue; B = 0.31, z = 1.22, p = .22. Hit rate also predicted looking at nothing in congruent cue condition only; B = 0.05, t = 2.29, p = .02. Looks to relevant, blank locations did not predict hit latency in neither of the cue condition (congruent cue; B = -44,74, t = 0.66, p = .51, no cue; B = 53.56, t = 0.70 p = .48). Interaction between Corsi-block tapping score and fixation percentage in the relevant quadrant did not predict hit rate or hit latency (ps &gt; .05). 3.6 Discussion The purpose of the current study is to shed light on the nature of spatial indexing and looking at nothing mechanisms and particularly, to investigate the relationship between internal and external memory within memory for language. To this end, we asked three questions as to automaticity of spatial indexing, dynamicity of spatial indexing and looking at nothing and the effect of individual differences in visuospatial memory on looking behaviour. 3.6.1 Looking at previous word locations Results showed there were significantly more fixations in the relevant, blank region where the probe word appeared at the encoding stage relative to other, irrelevant blank regions during memory retrieval. In other words, participants looked at nothing when retrieving simultaneously and visually presented single words. Our results in the congruent and no cue conditions were in line with the previous studies evidencing looking at nothing when remembering verbal information (Hoover &amp; Richardson, 2008; D. C. Richardson &amp; Kirkham, 2004; D. C. Richardson &amp; Spivey, 2000; Scholz et al., 2018, 2011, 2016). That is, we replicated the corresponding area effect (Wantz et al., 2015). The novelty of this study lies in the linguistic information to be retrieved and how it is encoded and remembered. As discussed in introduction, people saw fact-teller objects along with the spoken information in the previous studies which document looking at nothing for language memory (e.g., Richardson &amp; Spivey, 2000). Thus, participants associated verbal information with external visual information. Such an explicit indexing might have motivated participants to rely on the environmental sources. Whereas, linguistic information was not explicitly associated with any visual object in the current study. Further, words appeared in the four cells of the grid at the same time (see Vankov, 2011). Lastly, memory was not probed with details about factual information or correct/incorrect statements but in a simple recognition memory test. In such a relatively minimal and ecologically valid retrieval scenario, participants offloaded memory work onto the environment by simulating locations unintentionally when retrieving linguistic information from memory. It is also important note that looking at nothing in the present study occurred following an intervening task (i.e., judging a maths equation). Simulating spatial locations following a demanding task might suggest that looking at nothing is a not mere residual of the encoding process but rather, an efficient means of memory retrieval (see Renkewitz &amp; Jahn, 2012). To put in a nutshell, our findings suggest that looking at nothing could be a more robust and ubiquitous behaviour than previously documented. One limitation of the current study could be the use of boxes. We aimed to enrich the spatial context on the screen by following the methodology in Spivey and Geng (2000) by placing words in rectangular boxes at the encoding stage in both experiments. Importantly, participants were asked to remember the probe word while looking at a retrieval screen with boxes without the words in them. This methodology allowed us to identify narrower and thus, more specific interest areas (i.e., boxes) than quadrants of the grid. However, to what extent remembering words while looking at a screen with empty boxes meets the definition of looking at nothing in strict terms can be discussed. Replication studies are necessary to ascertain that word locations can be registered, simulated and referred back to via eye movements without any contextual enrichment such as boxes (see Chapter 6 for a methodology where words are not placed in boxes). 3.6.2 Indexing word locations Visuospatial cues affected spatial indexing and thus, looking at nothing in line with our predictions. Participants looked at relevant, blank locations in congruent cue condition, that is, when the cue appeared in the same location as to the probe word. Importantly, there were also more looks in the relevant quadrant when there was no cue between encoding and retrieval stages (pure looking at nothing). Findings from no cue condition suggest that looking at nothing is not driven by mere attentional shift. Rather, eye movements in the present study resulted from the spatial indices associated with words and thus, were governed by memory for language. On the other hand, looking at nothing did not occur when the visuospatial cue appeared in a diagonal location as to the original location of the probe word (i.e., incongruent cue condition). Results indicate that participants formed spatial indices corresponding to simultaneously presented single words even though locational information was not required in the memory task. Spatial indices were formed for subsequent cues as well. Emergence and magnitude of looking at nothing were determined by the relationship between the spatial indices for words and cues. Congruent cues reinforced the encoded locations and amplified the corresponding area effect as expected. In turn, participants looked more at the relevant locations in congruent cue condition as compared to no cue condition. In contrast, incongruent cue functioned as interference. When spatial indices associated with words and visual cues did not match, the initial index attached to the word was updated. Consequently, eye movements to the relevant, blank location were disrupted. It is important to note that participants did not look at any blank region (relevant, irrelevant or centre) more than the other regions in incongruent cue condition. Such a behaviour suggests that spatial codes corresponding to words and visuospatial cues were in competition when they did not refer to the same location. We can conclude that word locations were registered in all cases. Participants were given only 1600 ms to study four words leaving 400 ms for each word. Thus, we can argue that word locations were encoded almost instantaneously upon the presentation. Further, locations were encoded unintentionally suggested by the fact that participants were naïve to the purpose of the experiment and they were not instructed to remember word locations (cf., Andrade &amp; Meudell, 1993; Naveh-Benjamin, 1988). Informal interviews with the participants after the experiment suggested that locations were indexed without awareness. In keeping with this, it appears safe to argue that spatial indexing mechanism in the current study meets most of the automaticity criteria (Moors &amp; De Houwer, 2006). In this regard, our results contrast with Pezdek et al. (1986), which shows automatic spatial encoding for objects but not words. In conclusion, we present that not only the existence but also the magnitude of looking at nothing is determined by the strength and stability of spatial encoding. Although spatial indexing and looking at nothing are inherently different processes, they are both linked to each other in a dynamic relationship. 3.6.3 Looking at nothing and visuospatial memory The chief finding of the study is the relation between visuospatial memory capacity and the tendency to look at blank locations. To our knowledge, this is the first direct evidence showing individual differences in looking at nothing. We showed this relation in predictive and correlational analyses. There was a positive correlation between visuospatial memory measured with Corsi-block tapping test and fixation percentages in the central interest area during retrieval in all cue conditions. Higher visuospatial memory predicted less looking at nothing under incongruent cue condition. In line, there were negative correlations between visuospatial memory and fixations in relevant (within incongruent cue condition) and irrelevant locations (within no cue condition). Taken together, participants with better visuospatial memory, thus richer internal sources, looked more at the centre of the screen rather than looking at relevant (or irrelevant) locations. Central interest area was the initial and thus, default looking position prompted by a central fixation cross shown before each trial. Given that participants head was stabilised on the chinrest, we assume that participants with better visuospatial memory who looked at the centre of the screen; in fact, did not look at any specific area. In other words, they sustained their attention on the internal sources rather than the external codes in space by not launching fixations to relevant, or as a matter of fact, irrelevant regions. Such a looking behaviour can be comparable to cases in which individuals avert their gazes (Glenberg, Schroeder, &amp; Robertson, 1998) or close their eyes (Vredeveldt, Hitch, &amp; Baddeley, 2011) in order to disengage from the environment in the face of cognitive difficulty. Here, we surmise that participants with better visuospatial memory did not feel the necessity to rely on the blank locations as their internal memory was sufficient to retrieve the probe word accurately. Thus, they did not look at any regions in a task where moving their eyes could drain cognitive sources further (see Scholz et al., 2018). This interpretation was supported by the fact that participants with better visuospatial memory did better in the memory test in general. Further, participants with better visuospatial memory looked less at nothing when they saw an incongruent cue. The negative correlation between visuospatial memory and fixations in the relevant quadrant within incongruent cue condition illuminates another dimension of the coordination between internal and external memory. We argue that additional and incongruent visuospatial information made the environment unreliable for a successful memory retrieval. In the event of such spatial interference, participants with better visuospatial memory seemed to ignore any deictic code either attached to words or cues and, turned to internal sources. It appears that unreliability of the external memory was detected as a function of the strength of internal visuospatial memory. Overall, findings support the integrated memory account (Ferreira et al., 2008; D. C. Richardson et al., 2009) where internal memory representations and spatial indices which are internalised with eye movements work cooperatively to realise fast and efficient retrieval. On the other hand, results are at odds with the view that looking at nothing is an automatic attempt to access contents of the spatial index (Spivey et al., 2004). If looking at nothing were an automatic behaviour as spatial indexing, all participants would be expected to display the same behaviour regardless of their memory capacity. Rather, results demonstrate that looking at nothing systematically changes not only with the task conditions (e.g., memory demands coming from the task difficulty) (Scholz et al., 2011; Wantz et al., 2015), encoding conditions (e.g., explicit/implicit spatial indexing), or retrieval conditions (e.g., type of retrieval questions, grid arrangement) (Spivey &amp; Geng, 2000) but also cognitive differences between individuals. Coordination between internal and external memory in looking at nothing presents further evidence for the dynamicity account of looking at nothing. On a larger scale, findings extend the literature showing that the likelihood of cognitive offloading is determined by the abundance of internal sources (Risko &amp; Dunn, 2015). One important aspect here is consciousness. Previous studies showing more frequent cognitive offloading as a consequence of worse internal capacity typically offers offloading as an option to the participants (e.g., Risko &amp; Dunn, 2015). However, looking at nothing is an unintentional and presumably an unconscious behaviour in that participants in our study (and previous looking at nothing studies reviewed above) were never instructed to pay attention to word locations and that they can rely on the environment whenever they encountered retrieval difficulty. Even though, they still used the environment in an intelligent way (Kirsh, 1995) and further, this behaviour was modulated by their internal capacity. Such an unintentional trade-off between internal and external memory might suggest that cognitive offloading to minimise memory load could be a deeply-entrenched but an unconscious memory strategy. That said, consciousness and intentionality were not systematically tested in the current study. Future studies should be designed in a way to investigate whether looking at nothing is a completely unconscious behaviour, or whether we have some kind of control on our decision to offload memory work onto the world. 3.6.4 Looking at nothing and memory performance Results showed that participants who looked at the relevant, blank locations retrieved the probe words more accurately only in congruent cue condition. Accuracy predicted more looks in the relevant quadrant within congruent cue condition as well (see Martarelli et al., 2017; Martarelli &amp; Mast, 2011; Scholz et al., 2014 for looking at nothing occuring in correct trials but not in error trials). Looking at nothing did not predict hit rate in the no cue condition or hit latency in none of the conditions. Thus, we did not present any conclusive evidence that looking at nothing improves memory performance. It can well be argued that Simon-like congruency effect (as in congruent condition) accounts for the enhanced memory rather than fixations in the relevant, blank quadrant. Along with that, eye movements at retrieval were not manipulated in the current study unlike previous studies (e.g., Scholz et al., 2014). Hence, our method to investigate the functionality of gaze position lacks direct causality between looking at nothing and accuracy. Consequently, results reported here cannot distinguish whether participants who looked at nothing were more accurate or participants who were more accurate also looked at nothing. Findings showing partial functionality in the present work should be interpreted cautiously due to methodological limitations. 3.7 Conclusion Looking at nothing is a unique case in that it demonstrates how the cognitive system can maximize efficiency by spreading the cognitive problem across three domains with the act of looking, the environment with the spatial indices and mental representations in the brain. Our results extended the current literature by shedding further light on the nature of spatial indexing and looking at nothing mechanisms. We provide evidence for automatic and dynamic spatial indexing and a dynamic, efficient looking at nothing behaviour for words. The major contribution of this study is showing a systematic trade-off between internal and external sources driven by individual cognitive differences in order to make the most of environmental opportunities and cognitive capacity. Finally, the current looking at nothing paradigm provides a venue to study the relationship between language and looking at nothing. The same analyses were performed with dwell time percentages as well and findings were consistent with the analyses based on fixation percentages reported here. "]]
